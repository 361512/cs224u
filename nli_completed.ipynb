{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language inference\n",
    "\n",
    "Chris Potts, for [Stanford's CS224u: Natural language understanding](https://web.stanford.edu/class/cs224u/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of NLP/NLU, Natural Language Inference (NLI) is the task of predicting the logical\n",
    "relationships between words, phrases, sentences, (paragraphs, documents, ...). Such relationships\n",
    "are crucial for all kinds of reasoning in natural language: arguing, debating, problem solving, \n",
    "summarization, extrapolation, and so forth. \n",
    "\n",
    "NLI is a great task for this course. It requires serious linguistic analysis to do well, \n",
    "there are good (albeit small) publicly available data sets, and there are some natural baselines\n",
    "that help with getting a model up and running, and with understanding the performance of more \n",
    "sophisticated approaches. \n",
    "\n",
    "NLI was also the topic of [Bill's thesis](http://nlp.stanford.edu/~wcmac/papers/nli-diss.pdf) \n",
    "(he popularized the name \"NLI\"), so you can forever endear yourself to him by working on it!\n",
    "\n",
    "The purpose of this codebook is to introduce the problem of NLI more fully in the context of \n",
    "the [SemEval 2014 semantic relatedness task](http://alt.qcri.org/semeval2014/task1/). \n",
    "This data set is called \"Sentences Involving Compositional Knowledge\" or, for better or worse, \n",
    "\"SICK\". It's [freely available from the SemEval site](http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools), \n",
    "but we're going to work with a parsed version created by [Sam Bowman](http://stanford.edu/~sbowman/) \n",
    "as part of [his research on neural models of semantic composition](https://github.com/sleepinyourhat/vector-entailment/releases/tag/W15-R1). \n",
    "This data is in the subfolder `nli-data` of this Github repository.\n",
    "\n",
    "This codebook explores two general approaches: a standard classifier, and \n",
    "a shallow neural network using distributed representations.\n",
    "\n",
    "The [Classifier training and assessment](#Classifier-training-and-assessment) section also \n",
    "serves as a general illustration of how to take advantage of the \n",
    "[scikit-learn](http://scikit-learn.org/stable/) functions for \n",
    "doing feature selection, cross-validation, hyper-parameter optimization, and evaluation in the \n",
    "context of multi-class classification. That code could be easily modified to work with \n",
    "any classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. [Working with the parsed SICK data](#Working-with-the-parsed-SICK-data)\n",
    "   0. [Trees](#Trees)\n",
    "   0. [Readers](#Readers)\n",
    "0. [MaxEnt classifier approach](#MaxEnt-classifier-approach)\n",
    "   0. [Baseline classifier features](#Baseline-classifier-features)\n",
    "   0. [Classifier training and assessment](#Classifier-training-and-assessment)\n",
    "   0. [A few ideas for better classifier features](#A-few-ideas-for-better-classifier-features)\n",
    "0. [Shallow neural network approach](#Shallow-neural-network-approach)\n",
    "   0. [Baseline distributed features](#Baseline-distributed-features)                      \n",
    "   0. [Output label vectors](#Output-label-vectors) \n",
    "   0. [Network training and assessment](#Network-training-and-assessment)\n",
    "   0. [Potential next steps](#Potential-next-steps)\n",
    "0. [Some further reading](#Some-further-reading)\n",
    "0. [Homework 4](#Homework-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import copy\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "try:\n",
    "    import sklearn\n",
    "except ImportError:\n",
    "    sys.stderr.write(\"scikit-learn version 0.16.* is required\\n\")\n",
    "    sys.exit(2)\n",
    "if sklearn.__version__[:4] != '0.16':\n",
    "    sys.stderr.write(\"scikit-learn version 0.16.* is required. You're at %s.\\n\" % sklearn.__version__)\n",
    "    sys.exit(2)\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_selection import SelectFpr, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "from distributedwordreps import build, ShallowNeuralNetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the parsed SICK data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our parsed version of the SICK data contains triples like this (tab-separated fields):\n",
    "    \n",
    "`\n",
    "ENTAILMENT  ( ( A child ) ( is ( playing ( in ( a yard ) ) ) ) )  ( ( A child ) ( is playing ) )\n",
    "`\n",
    "\n",
    "`\n",
    "NEUTRAL  ( ( A child ) ( is ( playing ( in ( a yard ) ) ) ) )  ( ( A child ) ( is ( wearing ( blue jeans ) ) ) )\n",
    "`\n",
    "\n",
    "`\n",
    "CONTRADICTION  ( ( A child ) ( is ( playing ) ) )  ( ( A child ) ( is sleeping ) )\n",
    "`\n",
    "\n",
    "The brackets encode a label-free constituency structure of each sentence. The three labels on the left \n",
    "are the classes that we want to learn to predict. We'll frequently need access to them, so let's define\n",
    "them as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABELS = ['ENTAILMENT', 'CONTRADICTION', 'NEUTRAL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline models that I define here ignore all of the tree structure, but you'll likely want \n",
    "to take advantage of it. So the following function can be used to turn bracketed strings like\n",
    "the above into tuples of tuples encoding the syntactic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORD_RE = re.compile(r\"([^ \\(\\)]+)\", re.UNICODE)\n",
    "\n",
    "def str2tree(s):\n",
    "    \"\"\"Turns labeled bracketing s into a tree structure (tuple of tuples)\"\"\"\n",
    "    s = WORD_RE.sub(r'\"\\1\",', s)\n",
    "    s = s.replace(\")\", \"),\").strip(\",\")\n",
    "    s = s.strip(\",\")\n",
    "    return eval(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__': # Prevent this example from loading on import of this module.\n",
    "    \n",
    "    print str2tree(\"( ( A child ) ( is ( playing ( in ( a yard ) ) ) ) )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For baseline models, we often want just the words, also called terminal nodes or _leaves_.\n",
    "This function gives us access to them as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaves(t):\n",
    "    \"\"\"Returns all of the words (terminal nodes) in tree t\"\"\"\n",
    "    words = []\n",
    "    for x in t:\n",
    "        if isinstance(x, str):\n",
    "            words.append(x)\n",
    "        else:\n",
    "            words += leaves(x)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__': # Prevent this example from loading on import of this module.\n",
    "    \n",
    "    t = str2tree(\"( ( A child ) ( is ( playing ( in ( a yard ) ) ) ) )\")\n",
    "    print leaves(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easy to run through the corpus, let's define general readers for the SICK\n",
    "data. The general function for this yields triples consisting of the label, the left\n",
    "tree, and the right tree, as parsed by `str2tree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'nli-data/'\n",
    "\n",
    "def sick_reader(src_filename):\n",
    "    for example in csv.reader(file(src_filename), delimiter=\"\\t\"):\n",
    "        label, t1, t2 = example[:3]\n",
    "        if not label.startswith('%'): # Some files use leading % for comments.           \n",
    "            yield (label, str2tree(t1), str2tree(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define separate readers for the training and development data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sick_train_reader():\n",
    "    return sick_reader(src_filename=data_dir+\"SICK_train_parsed.txt\")\n",
    "\n",
    "def sick_dev_reader():\n",
    "    return sick_reader(src_filename=data_dir+\"SICK_trial_parsed.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, we'll want a test-set reader. As Bill discussed in class, though, \n",
    "__we swear on our honor as scholars that we won't use this data until\n",
    "system development is complete and we are ready to conduct our final\n",
    "assessment__!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sick_test_reader():\n",
    "    return sick_reader(src_filename=data_dir+\"SICK_test_parsed.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxEnt classifier approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline classifier features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first baseline we define is the _word overlap_ baseline. It simply uses as\n",
    "features the words that appear in both sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_overlap_features(t1, t2):\n",
    "    overlap = [w1 for w1 in leaves(t1) if w1 in leaves(t2)]\n",
    "    return Counter(overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular baseline is to use as features the full cross-product of\n",
    "words from both sentences:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_cross_product_features(t1, t2):\n",
    "    return Counter([(w1, w2) for w1, w2 in itertools.product(leaves(t1), leaves(t2))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these feature functions return count dictionaries mapping feature names to \n",
    "the number of times they occur in the data. This is the representation we'll work\n",
    "with throughout; scikit-learn will handle the further processing it needs to build\n",
    "linear classifiers.\n",
    "\n",
    "Naturally, you can do better than these feature functions! \n",
    "Both of these feature classes might be useful even in a more advanced model, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier training and assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in training a classifier is using a feature function like the one above\n",
    "to turn the data into a list of _training instances_: feature representations and their \n",
    "associated labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurizer(reader=sick_train_reader, feature_function=word_overlap_features):\n",
    "    \"\"\"Map the data in reader to a list of features according to feature_function,\n",
    "    and create the gold label vector.\"\"\"\n",
    "    feats = []\n",
    "    labels = []\n",
    "    split_index = None\n",
    "    for label, t1, t2 in reader():\n",
    "        d = feature_function(t1, t2)\n",
    "        feats.append(d)\n",
    "        labels.append(label)              \n",
    "    return (feats, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main work of training a model is done by the following `train_classifier` method. At a high-level, \n",
    "this function is doing a few things, with the goal of finding the right model (in the\n",
    "class of models we're exploring) for the data we have to train on. The major steps:                                                                        \n",
    "\n",
    "0. Use scikit-learn to turn the feature dictionaries into a big sparse matrix.\n",
    "0. Optionally use a bit of statistical testing to select features that seem discriminating. \n",
    "0. Use cross-validation within the training data to try to find the right regularization scheme.\n",
    "0. Train using the best-looking regularization scheme found in the previous step.\n",
    "0. Report cross-valided F1 scores to provide a sense for how we're doing.\n",
    "0. Return the fitted model plus the objects we need to align with future evaluation data.\n",
    "\n",
    "Part of the thinking behind this approach is that one can work as follows:\n",
    "\n",
    "* By and large, you check models just by training them and seeing how they do in cross-validation.\n",
    "* Only ccasionally, and only with good reason, do you check how you're doing on the dev set. If this is done only rarely, then it will help prevent you from over-fitting to quirks of this data or the training data.\n",
    "* At the very end, one runs the test. \n",
    "* The final paper can report, for the final model,\n",
    "  * Mean cross-validation values on the training data with standard errors\n",
    "  * Dev set performance\n",
    "  * Test set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_classifier(\n",
    "        reader=sick_train_reader,\n",
    "        feature_function=word_overlap_features,\n",
    "        feature_selector=SelectFpr(chi2, alpha=0.05), # Use None to stop feature selection\n",
    "        cv=10, # Number of folds used in cross-validation\n",
    "        priorlims=np.arange(.1, 3.1, .1)): # regularization priors to explore (we expect something around 1)\n",
    "    # Featurize the data:\n",
    "    feats, labels = featurizer(reader=reader, feature_function=feature_function) \n",
    "    \n",
    "    # Map the count dictionaries to a sparse feature matrix:\n",
    "    vectorizer = DictVectorizer(sparse=False)\n",
    "    X = vectorizer.fit_transform(feats)\n",
    "\n",
    "    ##### FEATURE SELECTION    \n",
    "    # (An optional step; not always productive). By default, we select all\n",
    "    # the features that pass the chi2 test of association with the\n",
    "    # class labels at p < 0.05. sklearn.feature_selection has other\n",
    "    # methods that are worth trying. I've seen particularly good results\n",
    "    # with the model-based methods, which require some changes to the\n",
    "    # current code.\n",
    "    feat_matrix = None\n",
    "    if feature_selector:\n",
    "        feat_matrix = feature_selector.fit_transform(X, labels)\n",
    "    else:\n",
    "        feat_matrix = X\n",
    "    \n",
    "    ##### HYPER-PARAMETER SEARCH\n",
    "    # Define the basic model to use for parameter search:\n",
    "    searchmod = LogisticRegression(fit_intercept=True, intercept_scaling=1)\n",
    "    # Parameters to grid-search over:\n",
    "    parameters = {'C':priorlims, 'penalty':['l1','l2']}  \n",
    "    # Cross-validation grid search to find the best hyper-parameters:     \n",
    "    clf = GridSearchCV(searchmod, parameters, cv=cv)\n",
    "    clf.fit(feat_matrix, labels)\n",
    "    params = clf.best_params_\n",
    "\n",
    "    # Establish the model we want using the parameters obtained from the search:\n",
    "    mod = LogisticRegression(fit_intercept=True, intercept_scaling=1, C=params['C'], penalty=params['penalty'])\n",
    "\n",
    "    ##### ASSESSMENT              \n",
    "    # Cross-validation of our favored model; for other summaries, use different\n",
    "    # values for scoring: http://scikit-learn.org/dev/modules/model_evaluation.html\n",
    "    scores = cross_val_score(mod, feat_matrix, labels, cv=cv, scoring=\"f1_macro\")       \n",
    "    print 'Best model', mod\n",
    "    print '%s features selected out of %s total' % (feat_matrix.shape[1], X.shape[1])\n",
    "    print 'F1 mean: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std()*2)\n",
    "\n",
    "    # TRAIN OUR MODEL:\n",
    "    mod.fit(feat_matrix, labels)\n",
    "\n",
    "    # Return the trained model along with the objects we need to\n",
    "    # featurize test data in a way that aligns with our training\n",
    "    # matrix:\n",
    "    return (mod, vectorizer, feature_selector, feature_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train either of our baseline models. Here's the run for the word-overlap one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__': # Prevent this example from loading on import of this module.\n",
    "    \n",
    "    overlapmodel = train_classifier(feature_function=word_overlap_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The following code assess the output of `train_classifier` on new data. The default is to do this\n",
    "on the dev set, but this same code would be used to evaluate the final model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_trained_classifier(model=None, reader=sick_dev_reader):\n",
    "    \"\"\"Evaluate model, the output of train_classifier, on the data in reader.\"\"\"\n",
    "    mod, vectorizer, feature_selector, feature_function = model\n",
    "    feats, labels = featurizer(reader=reader, feature_function=feature_function)\n",
    "    feat_matrix = vectorizer.transform(feats)\n",
    "    if feature_selector:\n",
    "        feat_matrix = feature_selector.transform(feat_matrix)\n",
    "    predictions = mod.predict(feat_matrix)\n",
    "    return metrics.classification_report(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we do on the training data as well as on the development data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__': # Prevent this example from loading on import of this module.\n",
    "    \n",
    "    for readername, reader in (('Train', sick_train_reader), ('Dev', sick_dev_reader)):\n",
    "        print \"======================================================================\"\n",
    "        print readername\n",
    "        print evaluate_trained_classifier(model=overlapmodel, reader=reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `word_cross_product_features` model achieves better results, since it has more information, but it takes\n",
    "a while to train --- and look at how substantially the feature space is affected by \n",
    "feature selection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__': # Prevent this example from loading on import of this module.\n",
    "    \n",
    "    crossmodel = train_classifier(feature_function=word_cross_product_features)\n",
    "    \n",
    "    for readername, reader in (('Train', sick_train_reader), ('Dev', sick_dev_reader)):\n",
    "        print \"======================================================================\"\n",
    "        print readername\n",
    "        print evaluate_trained_classifier(model=crossmodel, reader=reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few ideas for better classifier features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cross product of synsets compatible with each word, as given by WordNet. (Here is [a codebook on using WordNet from NLTK to do things like this](http://compprag.christopherpotts.net/wordnet.html).)\n",
    "\n",
    "* More fine-grained WordNet features &mdash; e.g., spotting pairs like _puppy_/_dog_ across the two sentences.\n",
    "\n",
    "* Use of other WordNet relations (see Table 1 and Table 2 in [the above codelab](http://compprag.christopherpotts.net/wordnet.html) for relations and their coverage).\n",
    "\n",
    "* Using the tree structure to define features that are sensitive to how negation scopes over constituents.\n",
    "\n",
    "* Features that are sensitive to differences in negation between the two sentences.\n",
    "\n",
    "* Sentiment features seeking to identify contrasting polarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Shallow neural network approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline distributed features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline I define here just turns each sentence into the average of all its word vectors.\n",
    "To create inputs to the network, we just concatenate the output of `glove_featurizer` for the\n",
    "two trees being compared. Pretty simplistic, but it is at least a start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GLOVE_MAT, GLOVE_VOCAB, _ = build('../distributedwordreps-data/glove.6B.50d.txt', delimiter=' ', header=False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def glvvec(w):\n",
    "    \"\"\"Return the GloVe vector for w.\"\"\"\n",
    "    i = GLOVE_VOCAB.index(w)\n",
    "    return GLOVE_MAT[i]\n",
    "\n",
    "def glove_features(t):\n",
    "    \"\"\"Return the mean glove vector of the leaves in tree t.\"\"\"\n",
    "    return np.mean([glvvec(w) for w in leaves(t) if w in GLOVE_VOCAB], axis=0)\n",
    "\n",
    "def vec_concatenate(u, v):\n",
    "    return np.concatenate((u, v)) \n",
    "\n",
    "def glove_featurizer(t1, t2):\n",
    "    \"\"\"Combined input vector based on glove_features and concatenation.\"\"\"\n",
    "    return vec_concatenate(glove_features(t1), glove_features(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output label vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shoehorn the current problem into our current neural network implementation, I \n",
    "define our output vectors as having a 1 for the dimension of the correct class, and a -1 \n",
    "for the other two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labelvec(label):\n",
    "    \"\"\"Return output vectors like [1,-1,-1], where the unique 1 is the true label.\"\"\"\n",
    "    vec = np.repeat(-1.0, 3)\n",
    "    vec[LABELS.index(label)] = 1.0\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network training and assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network, we first create the training data by pairing\n",
    "our sentence-pair vectors with the output vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_prep(reader=sick_train_reader, featurizer=glove_featurizer):    \n",
    "    dataset = []\n",
    "    for label, t1, t2 in reader():     \n",
    "        dataset.append([featurizer(t1, t2), labelvec(label)])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is then straightforward: it just uses `ShallowNeuralNetwork`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(\n",
    "        hidden_dim=100, \n",
    "        maxiter=1000, \n",
    "        reader=sick_train_reader,\n",
    "        featurizer=glove_featurizer,\n",
    "        display_progress=False):  \n",
    "    dataset = data_prep(reader=reader, featurizer=featurizer)\n",
    "    net = ShallowNeuralNetwork(input_dim=len(dataset[0][0]), hidden_dim=hidden_dim, output_dim=len(LABELS))\n",
    "    net.train(dataset, maxiter=maxiter, display_progress=display_progress)\n",
    "    return (net, featurizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation function evaluates the output of `train_network` using a new data reader.\n",
    "The `featurizer` is return by `train_network` to ensure that it is used consistently in\n",
    "training and evalution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_trained_network(network=None, reader=sick_dev_reader):\n",
    "    \"\"\"Evaluate network, the output of train_network, on the data in reader\"\"\"\n",
    "    net, featurizer = network\n",
    "    dataset = data_prep(reader=reader, featurizer=featurizer)\n",
    "    predictions = []\n",
    "    cats = []\n",
    "    for ex, cat in dataset:            \n",
    "        # The raw prediction is a triple of real numbers:\n",
    "        prediction = net.predict(ex)\n",
    "        # Argmax dimension for the prediction; this could be done better with\n",
    "        # an explicit softmax objective:\n",
    "        prediction = LABELS[np.argmax(prediction)]\n",
    "        predictions.append(prediction)\n",
    "        # Store the gold label for the classification report:\n",
    "        cats.append(LABELS[np.argmax(cat)])        \n",
    "    # Report:\n",
    "    return metrics.classification_report(cats, predictions, target_names=LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a summary train/dev evaluation. The network has trouble dealing with the\n",
    "class imbalances, but it seems to be doing okay overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__': # Prevent this example from loading on import of this module.\n",
    "\n",
    "    network = train_network(hidden_dim=10, maxiter=1000, reader=sick_train_reader, featurizer=glove_featurizer)\n",
    "    \n",
    "    for readername, reader in (('Train', sick_train_reader), ('Dev', sick_dev_reader)):\n",
    "        print \"======================================================================\"\n",
    "        print readername\n",
    "        print evaluate_trained_network(network=network, reader=reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you wrote improved neural net optimization code for HW1, then it will pay to use that instead of the basic network given in `distributedwordreps.py`.\n",
    "\n",
    "* Consider using a softmax objective for the final layer of the network, and another (tanh, rectified linear, etc.) for the hidden layers.\n",
    "\n",
    "* In the word-entailment in-class bake-off, the winning teams used vector difference instead of vector concatenation for the inputs. It's worth trying this, though the output classes are different here, so a variant might be called for.\n",
    "\n",
    "* And of course it would be worth paying attention to the syntactic structuring by defining a recursive neural network. See Bowman et al. 2014 for an appropriate architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bowman, Samuel R.; Christopher Potts; and Christopher D. Manning. 2014. \n",
    "[Recursive neural networks for learning logical semantics](http://arxiv.org/abs/1406.1827). \n",
    "arXiv manuscript 1406.1827. \n",
    "\n",
    "Dagan, Ido; Oren Glickman; and  Bernardo Magnini. 2006. \n",
    "[The PASCAL recognising textual entailment challenge](http://eprints.pascal-network.org/archive/00001298/01/dagan_et_al_rte05.pdf).\n",
    "In J. Quinonero-Candela, I. Dagan, B. Magnini, F. d'Alché-Buc, ed., _Machine Learning Challenges_, \n",
    "177-190. Springer-Verlag.\n",
    "\n",
    "Icard, Thomas F. 2012. [Inclusion and exclusion in natural language](http://link.springer.com/article/10.1007%2Fs11225-012-9425-8). _Studia Logica_ 100(4): 705-725.\n",
    "\n",
    "MacCartney, Bill and Christopher D. Manning. 2009. \n",
    "[An extended model of natural logic](http://www.aclweb.org/anthology/W09-3714). \n",
    "In  _Proceedings of the Eighth International Conference on Computational Semantics_, 140-156. \n",
    "Tilburg, The Netherlands: Association for Computational Linguistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All four problems are required. The work is due by the start of class on May 20.\n",
    "\n",
    "__Important__: These questions ask you to do evaluations. Great perfomance is always nice, but you should not spend hours or days trying to achieve it just for the sake of this assignment. (If you're obsessed, then go for it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "This problem calls for two revisions to `train_classifier`:\n",
    "\n",
    "* `train_classifier` currently optimizes our multi-class problem using a 'one vs. rest' scheme (`multi_class=ovr`). Revise `train_classifier` so that the choice of value for `multi_class` is part of the hyper-parameter search. Be sure to look over the documentation for `sklearn.linear_model.LogisticRegression` &mdash; this kind of search requires another adjustment to how the model is set up. (Don't worry about overlooking something; it won't work unless you figure it out!)\n",
    "\n",
    "* `train_classifier` currently does feature selection outside of the context of the model we ultimately want to fit. Revise the function so that the user has the option of using [Feature ranking with recursive feature elimination as implemented by scikit-learn's `RFE` function](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE). This is a model-based approach to feature selection that is slow but can be powerful. (It's tricky to decide whether to run RFE before or after grid-search for the optimal parameters. I suggest before, since the proper regularization scheme will be heavily influenced by the number of features.)\n",
    "\n",
    "__Submit__:\n",
    "\n",
    "0. Your revised `train_classifier` function.\n",
    "0. A copy-and-paste of all the messages printed by your revised `train_classifier` when it is trained on `sick_train_reader` using `word_overlap_features` and `RFE` (`n_features_to_select=None, step=1, verbose=0`), with a grid-search that includes at least both values of `multi_class`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### Problem 1 solution ######\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "def train_classifier_revised(\n",
    "        reader=sick_train_reader,\n",
    "        feature_function=word_overlap_features,\n",
    "        feature_selector='RFE', \n",
    "        cv=10,\n",
    "        priorlims=np.arange(.1, 3.1, .1),\n",
    "        multi_class=['ovr', 'multinomial']): \n",
    "    # Featurize the data:\n",
    "    feats, labels = featurizer(reader=reader, feature_function=feature_function) \n",
    "    \n",
    "    # Map the count dictionaries to a sparse feature matrix:\n",
    "    vectorizer = DictVectorizer(sparse=False)\n",
    "    X = vectorizer.fit_transform(feats)\n",
    "\n",
    "    ##### FEATURE SELECTION    \n",
    "    feat_matrix = None\n",
    "    if feature_selector and feature_selector != 'RFE':\n",
    "        feat_matrix = feature_selector.fit_transform(X, labels)\n",
    "    elif feature_selector == 'RFE':\n",
    "        featuremod = LogisticRegression()\n",
    "        feature_selector = RFE(featuremod)\n",
    "        feat_matrix = feature_selector.fit_transform(X, labels)\n",
    "    else:\n",
    "        feat_matrix = X\n",
    "    \n",
    "    ##### HYPER-PARAMETER SEARCH\n",
    "    # Define the basic model to use for parameter search:\n",
    "    searchmod = LogisticRegression(fit_intercept=True, intercept_scaling=1, solver='lbfgs')\n",
    "    # Parameters to grid-search over:\n",
    "    parameters = {'C':priorlims, 'penalty':['l1','l2'], 'multi_class':multi_class}  \n",
    "    # Cross-validation grid search to find the best hyper-parameters:     \n",
    "    clf = GridSearchCV(searchmod, parameters, cv=cv)\n",
    "    clf.fit(feat_matrix, labels)\n",
    "    params = clf.best_params_\n",
    "\n",
    "    # Establish the model we want using the parameters obtained from the search:\n",
    "    mod = LogisticRegression(\n",
    "        fit_intercept=True, \n",
    "        intercept_scaling=1, \n",
    "        C=params['C'], \n",
    "        penalty=params['penalty'],\n",
    "        solver='lbfgs',\n",
    "        multi_class=params['multi_class'])\n",
    "\n",
    "    ##### ASSESSMENT                  \n",
    "    scores = cross_val_score(mod, feat_matrix, labels, cv=cv, scoring=\"f1_macro\")       \n",
    "    print 'Best model', mod\n",
    "    print '%s features selected out of %s total' % (feat_matrix.shape[1], X.shape[1])\n",
    "    print 'F1 mean: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std()*2)\n",
    "\n",
    "    # TRAIN OUR MODEL:\n",
    "    mod.fit(feat_matrix, labels)\n",
    "       \n",
    "    return (mod, vectorizer, feature_selector, feature_function)\n",
    "\n",
    "_ = train_classifier_revised(feature_function=word_overlap_features) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "[Python NLTK](http://www.nltk.org) has an excellent WordNet interface. (If you don't have NLTK installed, install it now!) As noted above, WordNet is a natural choice for defining useful features in the context of NLI.\n",
    "\n",
    "__Your task__: write a feature function, for use with `train_classifier`, that is just like `word_cross_product_features` except that, given a sentence pair $(S_{1}, S_{2})$, it counts only pairs $(w_{1}, w_{2})$ such that $w_{1}$ entails $w_{2}$, for $w_{1} \\in S_{1}$ and $w_{2} \\in S_{2}$. For example, the sentence pair (_the cat runs_, _the animal moves_) would create the dictionary `{(cat, animal): 1.0, (runs, moves): 1.0}`.\n",
    "\n",
    "There are many ways to do this. For the purposes of the question, we can limit attention to the WordNet hypernym relation. The following illustrates reasonable ways to go from a string $s$ to the set of all hypernyms of Synsets consistent with $s$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    from nltk.corpus import wordnet as wn\n",
    "    \n",
    "    puppies = wn.synsets('puppy')\n",
    "    print [h for ss in puppies for h in ss.hypernyms()]\n",
    "\n",
    "    # A more conservative approach uses just the first-listed \n",
    "    # Synset, which should be the most frequent sense:\n",
    "    print wn.synsets('puppy')[0].hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whatever your preferred approach, the logic is that $w_{1}$ entails $w_{2}$ if a Synset consistent with $w_{2}$ is in the hypernym set you define for $w_{1}$.\n",
    "\n",
    "__Submit__:\n",
    "\n",
    "0. Your feature function.\n",
    "\n",
    "0. Copy-and-paste of the printed output of `evaluate_trained_classifier`, using your feature function, trained on `sick_train_reader`, with your preferred settings, and evaluated on `sick_dev_reader`.\n",
    "\n",
    "For more on using the Python NLTK interface, see [these notes](http://compprag.christopherpotts.net/wordnet.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### Problem 2 solution ######\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def hypernym_features(t1, t2):\n",
    "    pairs = []\n",
    "    words1 = leaves(t1)\n",
    "    words2 = leaves(t2)\n",
    "    for w1, w2 in itertools.product(words1, words2):\n",
    "        hyps = [h for ss in wn.synsets(w1) for h in ss.hypernyms()]\n",
    "        syns = wn.synsets(w2)\n",
    "        if set(hyps) & set(syns):\n",
    "            pairs.append((w1, w2))\n",
    "    return Counter(pairs)\n",
    "\n",
    "model = train_classifier(feature_function=hypernym_features) \n",
    "print evaluate_trained_classifier(model=model, reader=sick_dev_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By and large, deleting subphrases from a phrase will make it more general. For instance, if we begin with _fat cat_ and remove _cat_, then we end up with something that is entailed by the original. This also holds at the phrasal level. For example,\n",
    "\n",
    "\n",
    "_( land ( in ( a field ) ) )_ \n",
    "\n",
    "entails \n",
    "\n",
    "_( land )_\n",
    "\n",
    "__Your tasks__: \n",
    "\n",
    "* Write a function that, given a tree (as given by `str2tree` above), returns a list of all of the subphrases of that tree. For example, given `( land ( in ( a field ) ) )` the return value should be the following (order irrelevant):\n",
    "\n",
    "  `\n",
    "  [( land ( in ( a field ) ) ), ( in ( a field ) ), ( a field ), land, in, a, field]\n",
    "  `\n",
    "\n",
    "\n",
    "* Use this function to write a feature function that, given two tree-structured inputs $S_{1}$ and $S_{2}$, returns the number of cases where a phrase in $S_{2}$ contains a phrase in $S_{1}$ (including the case where the two phrases are identical).\n",
    "\n",
    "__Submit__: \n",
    "\n",
    "0. The two functions you wrote for the above tasks.\n",
    "\n",
    "0. Copy-and-paste of the printed output of `evaluate_trained_classifier`, using your feature function, trained on `sick_train_reader`, with your preferred settings, and evaluated on `sick_dev_reader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### Problem 3 solution ######\n",
    "\n",
    "def all_subphrases(tree):\n",
    "    phrases = [tree]\n",
    "    if isinstance(tree, str):\n",
    "        return phrases\n",
    "    for subtree in tree:\n",
    "        phrases += all_subphrases(subtree)\n",
    "    return phrases\n",
    "\n",
    "def subphrase_features(t1, t2):\n",
    "    sub1 = set(all_subphrases(t1))\n",
    "    sub2 = set(all_subphrases(t2))\n",
    "    subphrases = len([ph for ph in sub2 if ph in sub1])\n",
    "    return {'subphrases': subphrases}   \n",
    "    \n",
    "t1 = str2tree('( land ( in ( a field ) ) )')\n",
    "t2 = str2tree('( land ( a field ) ) ')    \n",
    "print all_subphrases(t1)    \n",
    "print subphrase_features(t1, t2)\n",
    "\n",
    "model = train_classifier(feature_function=subphrase_features) \n",
    "print evaluate_trained_classifier(model=model, reader=sick_dev_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "\n",
    "Write a new function, comparable to `glove_featurizer`, that employs an alternative to `vec_concatenate` and an alternative to `glove_features`. Train a network with this function using `train_network` and evaluate it using `evaluate_trained_network`, with the default parameters &mdash; except of course for `featurizer`, which should be your new function.\n",
    "\n",
    "__Submit__:\n",
    "\n",
    "0. Your new function.\n",
    "\n",
    "0. Copy-and-paste of the report given by `evaluate_trained_network` from your training and evaluation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### Problem 4 solution ######\n",
    "\n",
    "# IMDB_GLOVE_MAT, IMDB_GLOVE_VOCAB would be created and stored separately.\n",
    "import pickle\n",
    "IMDB_GLOVE_MAT, IMDB_GLOVE_VOCAB = pickle.load(file('/Volumes/CHRIS/Documents/teaching/2014-2015/spring/224u/codelabs/imdb-glove.pickle'))\n",
    "\n",
    "def pmivec(w):\n",
    "    i = IMDB_GLOVE_VOCAB.index(w)\n",
    "    return IMDB_GLOVE_MAT[i]\n",
    "\n",
    "def pmi_features(t):\n",
    "    return np.mean([pmivec(w) for w in leaves(t) if w in IMDB_GLOVE_VOCAB], axis=0)\n",
    "\n",
    "def vec_difference(u, v):\n",
    "    return u - v\n",
    "\n",
    "def my_featurizer(t1, t2):\n",
    "    \"\"\"Combined input vector based on glove_features and concatenation.\"\"\"\n",
    "    return vec_difference(pmi_features(t1), pmi_features(t2))    \n",
    "\n",
    "network = train_network(featurizer=my_featurizer, display_progress=False)\n",
    "print evaluate_trained_network(network=network, reader=sick_dev_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
