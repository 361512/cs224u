{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level entailment with neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2016\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Contents\n",
    "\n",
    "0. [Overview](#Overview)\n",
    "0. [Set-up](#Set-up)\n",
    "0. [Data](#Data)\n",
    "0. [Neural network architecture](#Neural-network-architecture)\n",
    "0. [Shallow neural networks from scratch](#Shallow-neural-networks-from-scratch)\n",
    "0. [Input feature representation](#Input-feature-representation)\n",
    "    0. [Representing words](#Representing-words)\n",
    "    0. [Combining words into inputs](#Combining-words-into-inputs)\n",
    "0. [Building datasets for experiments](#Building-datasets-for-experiments)\n",
    "0. [Running experiments](#Running-experiments)\n",
    "0. [Shallow neural networks in TensorFlow](#Shallow-neural-networks-in-TensorFlow)\n",
    "0. [In-class bake-off](#In-class-bake-off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem__: For two words $w_{1}$ and $w_{2}$, predict $w_{1} \\subset w_{2}$ or $w_{1} \\supset w_{2}$. This is a basic, word-level version of the task of __Natural Language Inference__ (NLI).\n",
    "\n",
    "__Approach__: Shallow feed-forward neural networks. Here's a broad overview of the model structure and task:\n",
    "\n",
    "![fig/wordentail.png](fig/wordentail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Make sure your environment includes all the requirements for [the cs224u repository](https://github.com/cgpotts/cs224u), especially TensorFlow, which isn't included in the standard Anaconda distribution (but is [easily installed](https://anaconda.org/jjhelmus/tensorflow)).\n",
    "0. Make sure you have the [the Wikipedia 2014 + Gigaword 5 distribution](http://nlp.stanford.edu/data/glove.6B.zip) of  pretrained GloVe vectors downloaded and unzipped, and that `glove_home` below is pointing to it.\n",
    "0. Make sure `wordentail_data_filename` below is pointing to the full path for `wordentail_data.pickle`, which is included in the cs224u repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordentail_data_filename = 'wordentail_data.pickle'\n",
    "glove_home = \"glove.6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import cPickle as pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from numpy import dot, outer\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suggested by the task decription, the dataset consists of word pairs with a label indicating that the first entails the second or the second entails the first. \n",
    "\n",
    "The pickled data distribution is a tuple in which the first member is the vocabulary for the entire dataset and the second is a dictionary establishing train/test splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordentail_data = pickle.load(file(wordentail_data_filename))\n",
    "vocab, splits = wordentail_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of `splits` creates a single training set and two different test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'disjoint_vocab_test', 'train']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All three sets are disjoint in terms of word pairs. \n",
    "\n",
    "* The `test` vocab is a subset of the `train` vocab. So every word seen at test time was seen in training. \n",
    "\n",
    "* The `disjoint_vocab_test` split has a vocabulary that is totally disjoint from `train`. So none of the words are seen in training. \n",
    "\n",
    "* All the words are in the GloVe vocabulary.\n",
    "\n",
    "Each split is itself a dict mapping class names to lists of word pairs. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, -1.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits['train'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['polynesian', 'inhabitant'],\n",
       " ['wiper', 'worker'],\n",
       " ['argonaut', 'adventurer'],\n",
       " ['bride', 'relative'],\n",
       " ['aramean', 'semite']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits['train'][1.0][: 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class labels are `1.0` if the first word entails the second, and `-1.0` if the second entails the first. These labels are scaled to the particular neural models we'll be using, in particular, to the `tanh` activation functions they use by default. It's also worth noting that we'll be treating these labels using a one-dimensional output space, since they are completely complementary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SUBSET = 1.0    # Left word entails right, as in (hippo, mammal)\n",
    "SUPERSET = -1.0 # Right word entails left, as in (mammal, hippo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we'll use a simple shallow neural network  parameterized as follows:\n",
    "\n",
    "* A weight matrix $W^{1}$ of dimension $m \\times n$, where $m$ is the dimensionality of the input vector representations and $n$ is the dimensionality of the hidden layer.\n",
    "* A bias term $b^{1}$ of dimension $n$.\n",
    "* A weight matrix $W^{2}$ of dimension $n \\times p$, where $p$ is the dimensionality of the output vector.\n",
    "* A bias term $b^{2}$ of dimension $p$.\n",
    "\n",
    "The network is then defined as follows, with $x$ the input layer, $h$ the hidden layer of dimension $n$, and $y$ the output of dimension $1 \\times p$:\n",
    "\n",
    "$$h = \\tanh\\left(xW^{1} + b^{1}\\right)$$\n",
    "\n",
    "$$y = \\tanh\\left(hW^{2} + b^{2}\\right)$$\n",
    "\n",
    "We'll first implement this from scratch and then reimplement it in [TensorFlow](https://www.tensorflow.org). Our hope is that this will provide a firm foundation for your own exploration of neural models for NLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow neural networks from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving to TensorFlow, it's worth building up our simple shallow architecture from scratch, as a way to explore the concepts and avoid the dangers of black-box machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def d_tanh(z):\n",
    "    \"\"\"The derivative of the hyperbolic tangent function. \n",
    "    z should be a float or np-array.\"\"\"\n",
    "    return 1.0 - z**2\n",
    "\n",
    "def progress_bar(iteration, error):\n",
    "    \"\"\"Simple over-writing progress bar for tracking the speed and \n",
    "    trajectory of training.\"\"\"\n",
    "    sys.stderr.write('\\r')\n",
    "    sys.stderr.write('completed iteration %s; error is %s' % ((iteration+1), error))\n",
    "    sys.stderr.flush()\n",
    "\n",
    "class ShallowNeuralNetwork:\n",
    "    \"\"\"Fit a model f(f(xW1 + b1)W2 + b2)\"\"\"    \n",
    "    def __init__(self, \n",
    "            hidden_dim=40,\n",
    "            afunc=np.tanh, \n",
    "            d_afunc=d_tanh,\n",
    "            maxiter=100,\n",
    "            eta=0.05,\n",
    "            epsilon=1.5e-8,\n",
    "            display_progress=True):\n",
    "        \"\"\"All the parameters are set as attributes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_dim : int (default: 40)\n",
    "            Dimensionality of the hidden layer.\n",
    "            \n",
    "        afunc : vectorized activation function (default: np.tanh)\n",
    "            The non-linear activation function used by the \n",
    "            network for the hidden and output layers.\n",
    "            \n",
    "        d_afunc : vectorized activation function derivative (default: `d_tanh`)\n",
    "            The derivative of `afunc`. It does not ensure that this \n",
    "            matches `afunc`, and craziness will result from mismatches!\n",
    "\n",
    "        maxiter : int default: 100)\n",
    "            Maximum number of training epochs.\n",
    "            \n",
    "        eta : float (default: 0.05)\n",
    "            Learning rate.\n",
    "            \n",
    "        epsilon : float (default: 1.5e-8)\n",
    "            Training terminates if the error reaches this point (or \n",
    "            `maxiter` is met).\n",
    "                    \n",
    "        display_progress : bool (default: True)\n",
    "           Whether to use the simple over-writing `progress_bar`\n",
    "           to show progress.                    \n",
    "        \n",
    "        \"\"\"\n",
    "        self.input_dim = None  # Set by the training data.\n",
    "        self.output_dim = None # Set by the training data.\n",
    "        self.hidden_dim = hidden_dim        \n",
    "        self.afunc = afunc \n",
    "        self.d_afunc = d_afunc \n",
    "        self.maxiter = maxiter\n",
    "        self.eta = eta        \n",
    "        self.epsilon = epsilon\n",
    "        self.display_progress = display_progress\n",
    "                \n",
    "    def forward_propagation(self, ex): \n",
    "        \"\"\"Computes the forward pass. ex shoud be a vector \n",
    "        of the same dimensionality as self.input_dim.\n",
    "        No value is returned, but the output layer self.y\n",
    "        is updated, as are self.x and self.h\"\"\"        \n",
    "        self.x[ : -1] = ex # ignore the bias\n",
    "        self.h[ : -1] = self.afunc(dot(self.x, self.W1)) # ignore the bias\n",
    "        self.y = self.afunc(dot(self.h, self.W2))        \n",
    "        \n",
    "    def backward_propagation(self, y_):\n",
    "        \"\"\"Send the error signal back through the network.\n",
    "        y_ is the ground-truth label we compare against.\"\"\"\n",
    "        y_ = np.array(y_)       \n",
    "        self.y_err = (y_ - self.y) * self.d_afunc(self.y)\n",
    "        h_err = dot(self.y_err, self.W2.T) * self.d_afunc(self.h)\n",
    "        self.W2 += self.eta * outer(self.h, self.y_err)\n",
    "        self.W1 += self.eta * outer(self.x, h_err[:-1]) # ignore the bias\n",
    "        return np.sum(0.5 * (y_ - self.y)**2)\n",
    "\n",
    "    def fit(self, training_data): \n",
    "        \"\"\"The training algorithm. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        training_data : list\n",
    "            A list of (example, label) pairs, where `example`\n",
    "            and `label` are both np.array instances.\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        self.x : the input layer \n",
    "        self.h : the hidden layer\n",
    "        self.y : the output layer\n",
    "        self.W1 : dense weight connection from self.x to self.h\n",
    "        self.W2 : dense weight connection from self.h to self.y\n",
    "        \n",
    "        Both self.W1 and self.W2 have the bias as their final column.\n",
    "        \n",
    "        The following attributes are created here for efficiency but \n",
    "        used only in `backward_propagation`:\n",
    "        \n",
    "        self.y_err : vector of output errors\n",
    "        self.x_err : vector of input errors\n",
    "        \n",
    "        \"\"\"\n",
    "        # Dimensions determined by the data:\n",
    "        self.input_dim = len(training_data[0][0])\n",
    "        self.output_dim = len(training_data[0][1])\n",
    "        # Parameter initialization:\n",
    "        self.x = np.ones(self.input_dim+1)  # +1 for the bias                                         \n",
    "        self.h = np.ones(self.hidden_dim+1) # +1 for the bias        \n",
    "        self.y = np.ones(self.output_dim)        \n",
    "        self.W1 = utils.randmatrix(self.input_dim+1, self.hidden_dim)\n",
    "        self.W2 = utils.randmatrix(self.hidden_dim+1, self.output_dim)        \n",
    "        self.y_err = np.zeros(self.output_dim)\n",
    "        self.x_err = np.zeros(self.input_dim+1)\n",
    "        # SGD:\n",
    "        iteration = 0\n",
    "        error = sys.float_info.max\n",
    "        while error > self.epsilon and iteration < self.maxiter:            \n",
    "            error = 0.0\n",
    "            random.shuffle(training_data)\n",
    "            for ex, labels in training_data:\n",
    "                self.forward_propagation(ex)\n",
    "                error += self.backward_propagation(labels)           \n",
    "            if self.display_progress:\n",
    "                progress_bar(iteration, error)\n",
    "            iteration += 1\n",
    "                    \n",
    "    def predict(self, ex):\n",
    "        \"\"\"Prediction for `ex`, which must be featurized as the\n",
    "        training data were. Simply runs `foward_propagation` and\n",
    "        returns a copy of self.y.\"\"\"\n",
    "        self.forward_propagation(ex)\n",
    "        return copy.deepcopy(self.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input feature representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even in deep learning, feature representation is the most important thing and requires care!\n",
    "For our task, feature representation has two parts: representing the individual words and combining those representations into a single network input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline word representation will be random vectors. This works well for the `test` task but is of course hopeless for the `disjoint_vocab_test` one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randvec(w, n=40, lower=-0.5, upper=0.5):\n",
    "    \"\"\"Returns a random vector of length n. w is ignored.\"\"\"\n",
    "    return np.array([random.uniform(lower, upper) for i in range(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas random inputs are hopeless for `disjoint_vocab_test`, GloVe vectors might not be ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Any of the files in glove.6B will work here:\n",
    "glove_src = os.path.join(glove_home, 'glove.6B.50d.txt')\n",
    "\n",
    "GLOVE_MAT, GLOVE_VOCAB, _ = utils.build_glove(glove_src)\n",
    "\n",
    "def glvvec(w):\n",
    "    \"\"\"Return the GloVe vector for w.\"\"\"\n",
    "    i = GLOVE_VOCAB.index(w)\n",
    "    return GLOVE_MAT[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining words into inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we decide how to combine the two word vectors into a single representation. In more detail, where $x_{l}$ is a vector representation of the left word and $x_{r}$ is a vector representation of the right word, we need a function $\\textbf{combine}$ such that $\\textbf{combine}(x_{l}, x_{r})$ returns a new input vector $x$ of dimension $m$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec_concatenate(u, v):\n",
    "    \"\"\"Concatenate np.array instances u and v into a new np.array\"\"\"\n",
    "    return np.concatenate((u, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{combine}$ could be concatenation as in `vec_concatenate`, or vector average, vector difference, etc. (even combinations of those) &mdash; there's lots of space for experimentation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building datasets for experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we define a function that featurizes the data (here, according to `vector_func` and `vector_combo_func`) and puts it into the right format for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "        wordentail_data, \n",
    "        vector_func=randvec, \n",
    "        vector_combo_func=vec_concatenate): \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    wordentail_data\n",
    "        The pickled dataset at `wordentail_data_filename`\n",
    "    \n",
    "    vector_func : (default: `randvec`)\n",
    "        Any function mapping words in the vocab for `wordentail_data`\n",
    "        to vector representations\n",
    "        \n",
    "    vector_combo_func : (default: `vec_concatenate`)\n",
    "        Any function for combining two vectors into a new vector\n",
    "        of fixed dimensionality.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dataset : defaultdict\n",
    "        A map from class names (here 1.0 or -1.0) to sets of \n",
    "        training instances, where a training instance is a \n",
    "        [vector, vector] list, the first giving the input \n",
    "        representation and the second giving the output \n",
    "        representation: \n",
    "        \n",
    "        {'train': [(vec, [cls]), (vec, [cls]), ...],\n",
    "         'test':  [(vec, [cls]), (vec, [cls]), ...],\n",
    "         'disjoint_vocab_test': [(vec, [cls]), (vec, [cls]), ...]}\n",
    "    \n",
    "    \"\"\"\n",
    "    # Load in the dataset:\n",
    "    vocab, splits = wordentail_data\n",
    "    # A mapping from words (as strings) to their vector\n",
    "    # representations, as determined by vector_func:\n",
    "    vectors = {w: vector_func(w) for w in vocab}\n",
    "    # Dataset in the format required by the neural network:\n",
    "    dataset = defaultdict(list)\n",
    "    for split, data in splits.items():\n",
    "        for clsname, word_pairs in data.items():\n",
    "            for w1, w2 in word_pairs:\n",
    "                # Use vector_combo_func to combine the word vectors for\n",
    "                # w1 and w2, as given by the vectors dictionary above,\n",
    "                # and pair it with the singleton array containing clsname:\n",
    "                item = [vector_combo_func(vectors[w1], vectors[w2]), np.array([clsname])]\n",
    "                dataset[split].append(item)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `experiment` trains its `network` parameters on `dataset['train']` and then evaluates its performance on all three splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def experiment(dataset, network): \n",
    "    \"\"\"General experiment and evaluation code for the \n",
    "    word-level entailment task.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : dict\n",
    "        With keys 'train', 'test', and 'disjoint_vocab_test',\n",
    "        each with values that are lists of vector pairs, the\n",
    "        first giving the example representation and the \n",
    "        second giving its 1d output vector. The expectation\n",
    "        is that this was created by `build_dataset`.\n",
    "    \n",
    "    network\n",
    "        This will be `ShallowNeuralNetwork` or `TfShallowNeuralNetwork`\n",
    "        below, but it could be any function that can train and \n",
    "        evaluate on `dataset`. The needed methods are `fit` and\n",
    "        `predict`.\n",
    "    \n",
    "    Prints\n",
    "    ------\n",
    "    To standard ouput\n",
    "        An sklearn classification report for all three splits.\n",
    "        \n",
    "    \"\"\"  \n",
    "    # Train the network:\n",
    "    network.fit(dataset['train'])\n",
    "    # The following is evaluation code. You won't have to alter it\n",
    "    # unless you did something unexpected like transform the output\n",
    "    # variables before training.\n",
    "    for typ in ('train', 'test', 'disjoint_vocab_test'):\n",
    "        data = dataset[typ]\n",
    "        predictions = []\n",
    "        cats = []\n",
    "        for ex, cat in data:            \n",
    "            # The raw prediction is a singleton list containing a float,\n",
    "            # either -1 or 1. We want only its contents:\n",
    "            prediction = network.predict(ex)[0]\n",
    "            # Categorize the prediction for accuracy comparison:\n",
    "            prediction = SUPERSET if prediction <= 0.0 else SUBSET            \n",
    "            predictions.append(prediction)\n",
    "            # Store the gold label for the classification report:\n",
    "            cats.append(cat[0])\n",
    "        # Report:\n",
    "        print \"=\"*70\n",
    "        print typ\n",
    "        print classification_report(cats, predictions, target_names=['SUPERSET', 'SUBSET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a baseline experiment run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "completed iteration 100; error is 70.0674759164"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "train\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   SUPERSET       0.99      0.99      0.99      2000\n",
      "     SUBSET       0.99      0.99      0.99      2000\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4000\n",
      "\n",
      "======================================================================\n",
      "test\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   SUPERSET       0.90      0.87      0.88       200\n",
      "     SUBSET       0.87      0.90      0.89       200\n",
      "\n",
      "avg / total       0.89      0.89      0.88       400\n",
      "\n",
      "======================================================================\n",
      "disjoint_vocab_test\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   SUPERSET       0.47      0.47      0.47        49\n",
      "     SUBSET       0.47      0.47      0.47        49\n",
      "\n",
      "avg / total       0.47      0.47      0.47        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_dataset = build_dataset(\n",
    "    wordentail_data, \n",
    "    vector_func=randvec, \n",
    "    vector_combo_func=vec_concatenate)\n",
    "\n",
    "baseline_network = ShallowNeuralNetwork()\n",
    "\n",
    "experiment(baseline_dataset, baseline_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow neural networks in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now translate `ShallowNeuralNetwork` into TensorFlow. TensorFlow is a powerful library for building deep learning models. In essence, you define the model architecture and it handles the details of optimization. In addition, it is very high-performance, so it will scale to large datasets and complicated model designs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TfShallowNeuralNetwork:\n",
    "    \"\"\"Fairly exact reproduction of `ShallowNeuralNetwork` in\n",
    "    TensorFlow, differing only in some details of optimization.\"\"\"\n",
    "    def __init__(self, hidden_dim=40, maxiter=100, eta=0.05):\n",
    "        \"\"\"All the parameters are set as attributes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_dim : int (default: 40)\n",
    "            Dimensionality of the hidden layer.                   \n",
    "\n",
    "        maxiter : int default: 100)\n",
    "            Maximum number of training epochs.\n",
    "            \n",
    "        eta : float (default: 0.05)\n",
    "            Learning rate.                 \n",
    "        \n",
    "        \"\"\"\n",
    "        self.input_dim = None\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = None\n",
    "        self.maxiter = maxiter\n",
    "        self.eta = eta            \n",
    "                \n",
    "    def fit(self, training_data):\n",
    "        \"\"\"The training algorithm. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        training_data : list\n",
    "            A list of (example, label) pairs, where `example`\n",
    "            and `label` are both np.array instances.\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        self.sess : the TensorFlow session\n",
    "        self.x : place holder for input data\n",
    "        self.h : the hidden layer\n",
    "        self.y : the output layer -- more like the full model here.\n",
    "        self.W1 : dense weight connection from self.x to self.h\n",
    "        self.b1 : bias\n",
    "        self.W2 : dense weight connection from self.h to self.y\n",
    "        self.b2 : bias\n",
    "        self.y_ : placeholder for training data\n",
    "                \n",
    "        \"\"\"\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        # Dimensions determined by the data:\n",
    "        self.input_dim = len(training_data[0][0])\n",
    "        self.output_dim = len(training_data[0][1])        \n",
    "        # Network initialization. For the inputs x, None in the first\n",
    "        # dimension allows us to train and evaluate on datasets\n",
    "        # of different size.\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.input_dim])\n",
    "        self.W1 = tf.Variable(tf.random_normal([self.input_dim, self.hidden_dim]))\n",
    "        self.b1 = tf.Variable(tf.random_normal([self.hidden_dim]))\n",
    "        self.W2 = tf.Variable(tf.random_normal([self.hidden_dim, self.output_dim]))\n",
    "        self.b2 = tf.Variable(tf.random_normal([self.output_dim]))\n",
    "        # Network structure. As before, we use tanh for both \n",
    "        # layers. This is not strictly necessary, and TensorFlow\n",
    "        # makes it easier to try different combinations because\n",
    "        # it calculates the derivatives for us.\n",
    "        self.h = tf.nn.tanh(tf.matmul(self.x, self.W1) + self.b1)\n",
    "        self.y = tf.nn.tanh(tf.matmul(self.h, self.W2) + self.b2)\n",
    "        # A place holder for the true labels. None in the first\n",
    "        # dimension allows us to train and evaluate on datasets\n",
    "        # of different size.\n",
    "        self.y_ = tf.placeholder(tf.float32, [None, self.output_dim])\n",
    "        # This defines the objective as one of reducing the \n",
    "        # one-half squared total error. This could easily \n",
    "        # be made into a user-supplied parameter to facilitate\n",
    "        # exploration of other costs. See\n",
    "        # https://www.tensorflow.org/versions/r0.7/api_docs/python/math_ops.html#reduction\n",
    "        cost = tf.reduce_sum(0.5 * (self.y_ - self.y)**2)\n",
    "        # Simple GradientDescent (as opposed to the stochastic version\n",
    "        # used by `ShallowNeuralNetwork`). For more options, see\n",
    "        # https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#optimizers\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.eta).minimize(cost)\n",
    "        # TF session initialization:   \n",
    "        init = tf.initialize_all_variables()\n",
    "        self.sess.run(init)        \n",
    "        # Train (for larger datasets, the epochs should be batched):\n",
    "        x, y_ = zip(*training_data)\n",
    "        for iteration in range(self.maxiter):            \n",
    "            optimizer.run(feed_dict={self.x: x, self.y_: y_})                       \n",
    "\n",
    "    def predict(self, ex):\n",
    "        \"\"\"Prediction for `ex`, which must be featurized as the\n",
    "        training data were. This runs the model (forward \n",
    "        propagation with self.x replaced by the single \n",
    "        example ex.)\"\"\"\n",
    "        return self.sess.run(self.y, feed_dict={self.x: [ex]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a baseline run with this new network, using `baseline_dataset` as created above for our other baseline experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "train\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   SUPERSET       0.94      0.93      0.94      2000\n",
      "     SUBSET       0.93      0.94      0.94      2000\n",
      "\n",
      "avg / total       0.94      0.94      0.94      4000\n",
      "\n",
      "======================================================================\n",
      "test\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   SUPERSET       0.83      0.83      0.83       200\n",
      "     SUBSET       0.83      0.83      0.83       200\n",
      "\n",
      "avg / total       0.83      0.83      0.83       400\n",
      "\n",
      "======================================================================\n",
      "disjoint_vocab_test\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   SUPERSET       0.62      0.53      0.57        49\n",
      "     SUBSET       0.59      0.67      0.63        49\n",
      "\n",
      "avg / total       0.60      0.60      0.60        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_tfnetwork = TfShallowNeuralNetwork()\n",
    "\n",
    "experiment(baseline_dataset, baseline_tfnetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class bake-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__The goal__: achieve the highest average F1 score __on disjoint_vocab_test__.\n",
    "\n",
    "__Notes__\n",
    "\n",
    "* You must train only on the `train` split. No outside training instances can be brought in. You can, though, bring in outside information via your input vectors, as long as this information is not from `test` or `disjoint_vocab_test`.\n",
    "\n",
    "* Since the evaluation is for `disjoint_vocab_test`, you're not going to get very far with random input vectors! A GloVe featurizer is defined above ([`glvvec`](#Representing-words)). Feel free to look around for new word vectors on the Web, or even train your own using our `vsm` notebook.\n",
    "\n",
    "* You're not required to stick to the network structures defined above. For instance, you could create deeper versions of them. As long as you have `fit` and `predict` methods with the same input and output types as our networks, you should be able to use `experiment`. Using `experiment` is not a requirement, though."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
