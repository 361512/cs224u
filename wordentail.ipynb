{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level entailment with neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2016\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import cPickle as pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from numpy import dot, outer\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordentail_data__filename = 'wordentail_data.pickle'\n",
    "glove_home = \"glove.6B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem__: For two words $w_{1}$ and $w_{2}$, predict $w_{1} \\subset w_{2}$ or $w_{1} \\supset w_{2}$\n",
    "\n",
    "__Approach__: Feed-forward neural networks\n",
    "\n",
    "![fig/wordentail.png](fig/wordentail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordentail_data = pickle.load(file(wordentail_data__filename))\n",
    "vocab, splits = wordentail_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'disjoint_vocab_test', 'train']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All three sets are disjoint. \n",
    "\n",
    "* The `test` vocab is a subset of the `train` vocab. So every word seen at test time was seen in training. \n",
    "\n",
    "* The `disjoint_test` has a vocabulary that is totally disjoint from `train`. So none of the words are seen in training. \n",
    "\n",
    "* All the words are in the GloVe vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class labels:\n",
    "SUBSET = 1.0    # Left word entails right, as in (hippo, mammal)\n",
    "SUPERSET = -1.0 # Right word entails left, as in (mammal, hippo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow neural network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our simple shallow neural network is parameterized as follows:\n",
    "\n",
    "* A weight matrix $W^{1}$ of dimension $m \\times n$, where $m$ is the dimensionality of the input vector representations and $n$ is the dimensionality of the hidden layer.\n",
    "* A bias term $b_{1}$ of dimension $m \\times 1$.\n",
    "* A weight matrix $W^{2}$ of dimension $n \\times p$, where $p$ is the dimensionality of the output vector.\n",
    "* A bias term $b_{2}$ of dimension $n \\times 1$.\n",
    "* A non-linear activation functions $f$. In our initial experiments, this is $\\tanh$.\n",
    "\n",
    "The network is then defined as follows, with $x$ the input layer, $h$ the hidden layer of dimension $n$, and $y$ the output of dimension $1 \\times p$:\n",
    "\n",
    "$$h = f\\left(xW^{1} + b^{1}\\right)$$\n",
    "\n",
    "$$y = f\\left(hW^{2} + b^{2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def d_tanh(z):\n",
    "    return 1.0 - z**2\n",
    "\n",
    "def progress_bar(iteration, error):\n",
    "    sys.stderr.write('\\r')\n",
    "    sys.stderr.write('completed iteration %s; error is %s' % ((iteration+1), error))\n",
    "    sys.stderr.flush()\n",
    "\n",
    "class ShallowNeuralNetwork:\n",
    "    def __init__(self, \n",
    "            input_dim=0, \n",
    "            hidden_dim=0, \n",
    "            output_dim=0, \n",
    "            afunc=np.tanh, \n",
    "            d_afunc=d_tanh,\n",
    "            maxiter=100,\n",
    "            eta=0.05,\n",
    "            epsilon=1.5e-8,\n",
    "            display_progress=True):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.afunc = afunc \n",
    "        self.d_afunc = d_afunc \n",
    "        self.maxiter = maxiter\n",
    "        self.eta = eta        \n",
    "        self.epsilon = epsilon\n",
    "        self.display_progress = display_progress\n",
    "                \n",
    "    def forward_propagation(self, ex):        \n",
    "        self.x[ : -1] = ex # ignore the bias\n",
    "        self.h[ : -1] = self.afunc(dot(self.x, self.W1)) # ignore the bias\n",
    "        self.y = self.afunc(dot(self.h, self.W2))\n",
    "        return copy.deepcopy(self.y)\n",
    "        \n",
    "    def backward_propagation(self, y_):\n",
    "        y_ = np.array(y_)       \n",
    "        self.y_err = (y_ - self.y) * self.d_afunc(self.y)\n",
    "        h_err = dot(self.y_err, self.W2.T) * self.d_afunc(self.h)\n",
    "        self.W2 += self.eta * outer(self.h, self.y_err)\n",
    "        self.W1 += self.eta * outer(self.x, h_err[:-1]) # ignore the bias\n",
    "        return np.sum(0.5 * (y_ - self.y)**2)\n",
    "\n",
    "    def fit(self, training_data):        \n",
    "        # Parameter initialization:\n",
    "        self.x = np.ones(self.input_dim+1)  # +1 for the bias                                         \n",
    "        self.h = np.ones(self.hidden_dim+1) # +1 for the bias        \n",
    "        self.y = np.ones(self.output_dim)        \n",
    "        self.W1 = utils.randmatrix(self.input_dim+1, self.hidden_dim)\n",
    "        self.W2 = utils.randmatrix(self.hidden_dim+1, self.output_dim)        \n",
    "        self.y_err = np.zeros(self.output_dim)\n",
    "        self.x_err = np.zeros(self.input_dim+1)\n",
    "        # SGD:\n",
    "        iteration = 0\n",
    "        error = sys.float_info.max\n",
    "        while error > self.epsilon and iteration < self.maxiter:            \n",
    "            error = 0.0\n",
    "            random.shuffle(training_data)\n",
    "            for ex, labels in training_data:\n",
    "                self.forward_propagation(ex)\n",
    "                error += self.backward_propagation(labels)           \n",
    "            if self.display_progress:\n",
    "                progress_bar(iteration, error)\n",
    "            iteration += 1\n",
    "                    \n",
    "    def predict(self, ex):\n",
    "        self.forward_propagation(ex)\n",
    "        return copy.deepcopy(self.y)\n",
    "        \n",
    "    def hidden_representation(self, ex):\n",
    "        self.forward_propagation(ex)\n",
    "        return self.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input feature representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $x_{l}$ is a vector representation of the left word and $x_{r}$ is a representation of the right word, we define a combination function $\\textbf{combine}$ such that $\\textbf{combine}(x_{l}, x_{r})$ returns a new input vector $x$ of dimension $1 \\times m$. $\\textbf{combine}$ could be concatenation, vector average, vector difference, etc. (even combinations of those) &mdash; there's lots of space for experimentation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randvec(w, n=40, lower=-0.5, upper=0.5):\n",
    "    \"\"\"Returns a random vector of length n. w is ignored.\"\"\"\n",
    "    return np.array([random.uniform(lower, upper) for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_src = os.path.join(glove_home, 'glove.6B.50d.txt')\n",
    "GLOVE_MAT, GLOVE_VOCAB, _ = utils.build_glove(glove_src)\n",
    "\n",
    "def glvvec(w):\n",
    "    \"\"\"Return the GloVe vector for w.\"\"\"\n",
    "    i = GLOVE_VOCAB.index(w)\n",
    "    return GLOVE_MAT[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec_concatenate(u, v):\n",
    "    return np.concatenate((u, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building datasets for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(wordentail_data,vector_func=randvec, vector_combo_func=vec_concatenate): \n",
    "    # Load in the dataset:\n",
    "    vocab, splits = wordentail_data\n",
    "    # Make vectors a mapping from words (as strings) to their vector\n",
    "    # representations, as determined by vector_func.\n",
    "    vectors = {w: vector_func(w) for w in vocab}\n",
    "    # Create a dataset in the format required by the neural network:\n",
    "    # {'train': [(vec, [cls]), (vec, [cls]), ...],\n",
    "    #  'test':  [(vec, [cls]), (vec, [cls]), ...] }\n",
    "    dataset = defaultdict(list)\n",
    "    for split, data in splits.items():\n",
    "        for clsname, word_pairs in data.items():\n",
    "            for w1, w2 in word_pairs:\n",
    "                # Use vector_combo_func to combine the word vectors for\n",
    "                # w1 and w2, as given by the vectors dictionary above,\n",
    "                # and pair it with the singleton array containing clsname.\n",
    "                item = [vector_combo_func(vectors[w1], vectors[w2]), np.array([clsname])]\n",
    "                dataset[split].append(item)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def experiment(dataset, network): \n",
    "    # Get the train and test sets from the dataset:\n",
    "    train = dataset['train']\n",
    "    test = dataset['test']\n",
    "    disjoint_vocab_test = dataset['disjoint_vocab_test']    \n",
    "    # Set these dimensions based on the data:\n",
    "    network.input_dim = len(train[0][0])\n",
    "    network.output_dim = len(train[0][1])    \n",
    "    # Train the network, with the number of iterations set you by you\n",
    "    # (make it a keyword argument to this function). You might want\n",
    "    # to use display_progress=True to track errors andd speed.\n",
    "    network.fit(train)\n",
    "    # The following is evaluation code. You won't have to alter it\n",
    "    # unless you did something unexpected like  transform the output\n",
    "    # variables before training.\n",
    "    for typ, data in (('train', train), ('test', test), ('disjoint_vocab_test', disjoint_vocab_test)):\n",
    "        predictions = []\n",
    "        cats = []\n",
    "        for ex, cat in data:            \n",
    "            # The raw prediction is a singleton list containing a float in (-1,1).\n",
    "            # We want only its contents:\n",
    "            prediction = network.predict(ex)[0]\n",
    "            # Categorize the prediction for accuracy comparison:\n",
    "            prediction = SUPERSET if prediction <= 0.0 else SUBSET            \n",
    "            predictions.append(prediction)\n",
    "            # Store the gold label for the classification report:\n",
    "            cats.append(cat[0])\n",
    "        # Report:\n",
    "        print \"======================================================================\"\n",
    "        print typ\n",
    "        print classification_report(cats, predictions, target_names=['SUPERSET', 'SUBSET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "completed iteration 100; error is 60.0332157301"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "train\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   SUPERSET       0.99      0.99      0.99      2000\n",
      "     SUBSET       0.99      0.99      0.99      2000\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4000\n",
      "\n",
      "======================================================================\n",
      "test\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   SUPERSET       0.87      0.89      0.88       200\n",
      "     SUBSET       0.88      0.87      0.88       200\n",
      "\n",
      "avg / total       0.88      0.88      0.88       400\n",
      "\n",
      "======================================================================\n",
      "disjoint_vocab_test\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   SUPERSET       0.40      0.35      0.37        49\n",
      "     SUBSET       0.43      0.49      0.46        49\n",
      "\n",
      "avg / total       0.42      0.42      0.42        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(wordentail_data, vector_func=randvec, vector_combo_func=vec_concatenate)\n",
    "\n",
    "network = ShallowNeuralNetwork(hidden_dim=40, maxiter=100, eta=0.05, display_progress=True)\n",
    "\n",
    "experiment(dataset, network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow neural network in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TensorFlowShallowNeuralNetwork:\n",
    "    def __init__(self, \n",
    "            input_dim=0, \n",
    "            hidden_dim=0, \n",
    "            output_dim=0,             \n",
    "            maxiter=100,\n",
    "            eta=0.05):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.maxiter = maxiter\n",
    "        self.eta = eta            \n",
    "                \n",
    "    def fit(self, training_data):\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        # Network initialization:\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.input_dim])\n",
    "        self.W1 = tf.Variable(tf.random_normal([self.input_dim, self.hidden_dim]))\n",
    "        self.b1 = tf.Variable(tf.random_normal([self.hidden_dim]))\n",
    "        self.W2 = tf.Variable(tf.random_normal([self.hidden_dim, self.output_dim]))\n",
    "        self.b2 = tf.Variable(tf.random_normal([self.output_dim]))\n",
    "        # Network structure:\n",
    "        self.h = tf.nn.tanh(tf.matmul(self.x, self.W1) + self.b1)\n",
    "        self.y = tf.nn.tanh(tf.matmul(self.h, self.W2) + self.b2)\n",
    "        self.y_ = tf.placeholder(tf.float32, [None, self.output_dim])\n",
    "        # Optimization:\n",
    "        mean_squared_error = tf.reduce_sum(0.5 * (self.y_-self.y)**2)\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.eta).minimize(mean_squared_error)\n",
    "        # Train:\n",
    "        init = tf.initialize_all_variables()\n",
    "        self.sess.run(init)        \n",
    "        x, y_ = zip(*training_data)\n",
    "        for iteration in range(self.maxiter):            \n",
    "            self.optimizer.run(feed_dict={self.x: x, self.y_: y_})                       \n",
    "\n",
    "    def predict(self, ex):\n",
    "         return self.sess.run(self.y, feed_dict={self.x: [ex]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "train\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   SUPERSET       0.95      0.93      0.94      2000\n",
      "     SUBSET       0.93      0.95      0.94      2000\n",
      "\n",
      "avg / total       0.94      0.94      0.94      4000\n",
      "\n",
      "======================================================================\n",
      "test\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   SUPERSET       0.82      0.84      0.83       200\n",
      "     SUBSET       0.84      0.81      0.83       200\n",
      "\n",
      "avg / total       0.83      0.83      0.83       400\n",
      "\n",
      "======================================================================\n",
      "disjoint_vocab_test\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   SUPERSET       0.54      0.51      0.53        49\n",
      "     SUBSET       0.54      0.57      0.55        49\n",
      "\n",
      "avg / total       0.54      0.54      0.54        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(wordentail_data, vector_func=randvec, vector_combo_func=vec_concatenate)\n",
    "tfnet = TensorFlowShallowNeuralNetwork(hidden_dim=20, maxiter=1000)\n",
    "experiment(dataset, tfnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep neural network in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
