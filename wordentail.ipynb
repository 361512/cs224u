{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level entailment with neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2016\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem__: For two words $w_{1}$ and $w_{2}$, predict $w_{1} \\subset w_{2}$ or $w_{1} \\supset w_{2}$. This is a basic, word-level version of the task of __Natural Language Inference__ (NLI).\n",
    "\n",
    "__Approach__: Shallow feed-forward neural networks. Here's a broad overview of the model structure and task:\n",
    "\n",
    "![fig/wordentail.png](fig/wordentail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Make sure your environment includes all the requirements for [the cs224u repository](https://github.com/cgpotts/cs224u).\n",
    "0. Download [the Wikipedia 2014 + Gigaword 5 distribution](http://nlp.stanford.edu/data/glove.6B.zip) of the pretrained GloVe vectors, unzip it, and put the resulting folder in the the same directory as this notebook. (If you want to put it somewhere else, change `glove_home` below.)\n",
    "0. Make sure `wordentail_data_filename` below is pointing to the full path for `wordentail_data.pickle`, which is included in the cs224u repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordentail_data_filename = 'wordentail_data.pickle'\n",
    "glove_home = \"glove.6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-f05fd983e9b5>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-f05fd983e9b5>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    0. [Neural network architecture](#Neural-network-architecture)\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Contents\n",
    "\n",
    "0. [Data](#Data)\n",
    "0. [Neural network architecture](#Neural-network-architecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import cPickle as pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from numpy import dot, outer\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suggested by the task decription, the dataset consists of word pairs with a label indicating that the first entails the second or the second entails the first. The pickled data distribution is a pair in which the first member is the vocabulary for the entire dataset and the second is a dictionary establishing train/test splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordentail_data = pickle.load(file(wordentail_data__filename))\n",
    "vocab, splits = wordentail_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of `splits` creates a single training set and two different test sets that create quite different tasks in the context of our neural architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splits.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All three sets are disjoint. \n",
    "\n",
    "* The `test` vocab is a subset of the `train` vocab. So every word seen at test time was seen in training. \n",
    "\n",
    "* The `disjoint_test` has a vocabulary that is totally disjoint from `train`. So none of the words are seen in training. \n",
    "\n",
    "* All the words are in the GloVe vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class labels are `1.0` if the first element entails the second and `-1.0` if the secod entails the first. These labels are scaled to the particular neural models we'll be using, in particular, to the `tanh` activation functions they use by default. It's also worth noting that we'll be treating these labels using a single dimensional output space, since they are completely complementary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SUBSET = 1.0    # Left word entails right, as in (hippo, mammal)\n",
    "SUPERSET = -1.0 # Right word entails left, as in (mammal, hippo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we'll use a simple shallow neural network  parameterized as follows:\n",
    "\n",
    "* A weight matrix $W^{1}$ of dimension $m \\times n$, where $m$ is the dimensionality of the input vector representations and $n$ is the dimensionality of the hidden layer.\n",
    "* A bias term $b_{1}$ of dimension $m \\times 1$.\n",
    "* A weight matrix $W^{2}$ of dimension $n \\times p$, where $p$ is the dimensionality of the output vector.\n",
    "* A bias term $b_{2}$ of dimension $n \\times 1$.\n",
    "\n",
    "The network is then defined as follows, with $x$ the input layer, $h$ the hidden layer of dimension $n$, and $y$ the output of dimension $1 \\times p$:\n",
    "\n",
    "$$h = \\tanh\\left(xW^{1} + b^{1}\\right)$$\n",
    "\n",
    "$$y = tanh\\left(hW^{2} + b^{2}\\right)$$\n",
    "\n",
    "We'll first implement this from scratch and then reimplement it in TensorFlow. Our hope is that this will provide a firm foundation for your own exploration of neural models for the NLI task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is a powerful library for building deep learning models. In essence, you define the model architecture and the details of optimization. In addition, it is very high-performance, so it will scale to large datasets and complicated model designs. So, we'll want to start using it shortly. However, before making that move, it's worth building up our simple shallow architecture from scratch, as a way to explore the concepts and avoid the dangers of black-box optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def d_tanh(z):\n",
    "    \"\"\"The derivative of the hyperbolic tangent function. \n",
    "    z should be a float or np-array.\"\"\"\n",
    "    return 1.0 - z**2\n",
    "\n",
    "def progress_bar(iteration, error):\n",
    "    \"\"\"Simple over-writing progress bar for tracking the speed\n",
    "    and trajectory of training.\"\"\"\n",
    "    sys.stderr.write('\\r')\n",
    "    sys.stderr.write('completed iteration %s; error is %s' % ((iteration+1), error))\n",
    "    sys.stderr.flush()\n",
    "\n",
    "class ShallowNeuralNetwork:\n",
    "    \"\"\"Fit a model f(f(xW1 + b1)W2 = b2)\"\"\"    \n",
    "    def __init__(self, \n",
    "            input_dim=0, \n",
    "            hidden_dim=0, \n",
    "            output_dim=0, \n",
    "            afunc=np.tanh, \n",
    "            d_afunc=d_tanh,\n",
    "            maxiter=100,\n",
    "            eta=0.05,\n",
    "            epsilon=1.5e-8,\n",
    "            display_progress=True):\n",
    "        \"\"\"All the parameters are set as attributes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim, hidden_dim, output_dim : int, int, int\n",
    "            The basic dimension of the network. input_dim\n",
    "            and output_dim must match the dimensions of the\n",
    "            training data. hidden_dim is free.\n",
    "            \n",
    "        afunc : vectorized activation function (default: np.tanh)\n",
    "            The non-linear activation function used by the \n",
    "            network for the hidden and output layers.\n",
    "            \n",
    "        d_afunc :  vectorized activation function derivative (default: `d_tanh`)\n",
    "            The derivative of `afunc`. It is not ensure that this \n",
    "            matches `afunc`, and craziness will result from mismatches.\n",
    "\n",
    "        maxiter : int default: 100)\n",
    "            Maximum number of training epochs\n",
    "            \n",
    "        eta : float (default: 0.05)\n",
    "            Learning rate.\n",
    "            \n",
    "        epsilon : float (default: 1.5e-8)\n",
    "            Training terminates if the error reaches this\n",
    "            point (or `maxiter` is met).\n",
    "                    \n",
    "        display_progress : bool (default: True)\n",
    "           Whether to use the simple over-writing `progress_bar`\n",
    "           to show progress.                    \n",
    "        \n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.afunc = afunc \n",
    "        self.d_afunc = d_afunc \n",
    "        self.maxiter = maxiter\n",
    "        self.eta = eta        \n",
    "        self.epsilon = epsilon\n",
    "        self.display_progress = display_progress\n",
    "                \n",
    "    def forward_propagation(self, ex): \n",
    "        \"\"\"Computes the forward pass. ex shoud be a vector \n",
    "        of the same dimensionality as self.input_dim.\n",
    "        No value is returned, but the output layer self.y\n",
    "        is updated, as are self.x and self.h\n",
    "        \n",
    "        \"\"\"        \n",
    "        self.x[ : -1] = ex # ignore the bias\n",
    "        self.h[ : -1] = self.afunc(dot(self.x, self.W1)) # ignore the bias\n",
    "        self.y = self.afunc(dot(self.h, self.W2))        \n",
    "        \n",
    "    def backward_propagation(self, y_):\n",
    "        \"\"\"Send the error signal back through the network.\n",
    "        y_ is the ground-truth label we compare against.\"\"\"\n",
    "        y_ = np.array(y_)       \n",
    "        self.y_err = (y_ - self.y) * self.d_afunc(self.y)\n",
    "        h_err = dot(self.y_err, self.W2.T) * self.d_afunc(self.h)\n",
    "        self.W2 += self.eta * outer(self.h, self.y_err)\n",
    "        self.W1 += self.eta * outer(self.x, h_err[:-1]) # ignore the bias\n",
    "        return np.sum(0.5 * (y_ - self.y)**2)\n",
    "\n",
    "    def fit(self, training_data): \n",
    "        \"\"\"The training algorithm. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        training_data : list\n",
    "            A list of (example, label) pairs, where `example`\n",
    "            and `label` are both np.array instances.\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        self.x : the input layer\n",
    "        self.h : the hidden layer\n",
    "        self.y : the output layer\n",
    "        self.W1 : dense weight connection from self.x to self.h\n",
    "        self.W2 : dense weight connection from self.h to self.y\n",
    "        \n",
    "        Both self.W1 and self.W2 include the bias column as their\n",
    "        final dimension.\n",
    "        \n",
    "        The following attributes are created here for efficiency\n",
    "        but used only in `backward_propagation`:\n",
    "        \n",
    "        self.y_err : vector of output errors\n",
    "        self.x_err : vector of input errors \n",
    "        \"\"\"\n",
    "        # Parameter initialization:\n",
    "        self.x = np.ones(self.input_dim+1)  # +1 for the bias                                         \n",
    "        self.h = np.ones(self.hidden_dim+1) # +1 for the bias        \n",
    "        self.y = np.ones(self.output_dim)        \n",
    "        self.W1 = utils.randmatrix(self.input_dim+1, self.hidden_dim)\n",
    "        self.W2 = utils.randmatrix(self.hidden_dim+1, self.output_dim)        \n",
    "        self.y_err = np.zeros(self.output_dim)\n",
    "        self.x_err = np.zeros(self.input_dim+1)\n",
    "        # SGD:\n",
    "        iteration = 0\n",
    "        error = sys.float_info.max\n",
    "        while error > self.epsilon and iteration < self.maxiter:            \n",
    "            error = 0.0\n",
    "            random.shuffle(training_data)\n",
    "            for ex, labels in training_data:\n",
    "                self.forward_propagation(ex)\n",
    "                error += self.backward_propagation(labels)           \n",
    "            if self.display_progress:\n",
    "                progress_bar(iteration, error)\n",
    "            iteration += 1\n",
    "                    \n",
    "    def predict(self, ex):\n",
    "        \"\"\"Prediction for `ex`, which must be featurized as the\n",
    "        training data were. Simply runs `foward_propagation` and\n",
    "        returns a copy of self.y.\"\"\"\n",
    "        self.forward_propagation(ex)\n",
    "        return copy.deepcopy(self.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input feature representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even in deep learning, feature representation is the most important thing and requires care!\n",
    "\n",
    "For our task, feature representation has two parts: representing the individual words and combining those representations into a single network input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline word representation will be random vectors. This works well for the `test` task but is of course hopeless for the `disjoint_test` one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randvec(w, n=40, lower=-0.5, upper=0.5):\n",
    "    \"\"\"Returns a random vector of length n. w is ignored.\"\"\"\n",
    "    return np.array([random.uniform(lower, upper) for i in range(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas random inputs are hopeless for `disjoint_test`, GloVe vectors might not be ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_src = os.path.join(glove_home, 'glove.6B.50d.txt')\n",
    "GLOVE_MAT, GLOVE_VOCAB, _ = utils.build_glove(glove_src)\n",
    "\n",
    "def glvvec(w):\n",
    "    \"\"\"Return the GloVe vector for w.\"\"\"\n",
    "    i = GLOVE_VOCAB.index(w)\n",
    "    return GLOVE_MAT[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we decide how to combine the two word vectors into a single representation. In more detail, where $x_{l}$ is a vector representation of the left word and $x_{r}$ is a representation of the right word, we need a combination function $\\textbf{combine}$ such that $\\textbf{combine}(x_{l}, x_{r})$ returns a new input vector $x$ of dimension $1 \\times m$. $\\textbf{combine}$ could be concatenation, vector average, vector difference, etc. (even combinations of those) &mdash; there's lots of space for experimentation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec_concatenate(u, v):\n",
    "    return np.concatenate((u, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building datasets for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(wordentail_data,vector_func=randvec, vector_combo_func=vec_concatenate): \n",
    "    # Load in the dataset:\n",
    "    vocab, splits = wordentail_data\n",
    "    # Make vectors a mapping from words (as strings) to their vector\n",
    "    # representations, as determined by vector_func.\n",
    "    vectors = {w: vector_func(w) for w in vocab}\n",
    "    # Create a dataset in the format required by the neural network:\n",
    "    # {'train': [(vec, [cls]), (vec, [cls]), ...],\n",
    "    #  'test':  [(vec, [cls]), (vec, [cls]), ...] }\n",
    "    dataset = defaultdict(list)\n",
    "    for split, data in splits.items():\n",
    "        for clsname, word_pairs in data.items():\n",
    "            for w1, w2 in word_pairs:\n",
    "                # Use vector_combo_func to combine the word vectors for\n",
    "                # w1 and w2, as given by the vectors dictionary above,\n",
    "                # and pair it with the singleton array containing clsname.\n",
    "                item = [vector_combo_func(vectors[w1], vectors[w2]), np.array([clsname])]\n",
    "                dataset[split].append(item)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def experiment(dataset, network): \n",
    "    # Get the train and test sets from the dataset:\n",
    "    train = dataset['train']\n",
    "    test = dataset['test']\n",
    "    disjoint_vocab_test = dataset['disjoint_vocab_test']    \n",
    "    # Set these dimensions based on the data:\n",
    "    network.input_dim = len(train[0][0])\n",
    "    network.output_dim = len(train[0][1])    \n",
    "    # Train the network, with the number of iterations set you by you\n",
    "    # (make it a keyword argument to this function). You might want\n",
    "    # to use display_progress=True to track errors andd speed.\n",
    "    network.fit(train)\n",
    "    # The following is evaluation code. You won't have to alter it\n",
    "    # unless you did something unexpected like  transform the output\n",
    "    # variables before training.\n",
    "    for typ, data in (('train', train), ('test', test), ('disjoint_vocab_test', disjoint_vocab_test)):\n",
    "        predictions = []\n",
    "        cats = []\n",
    "        for ex, cat in data:            \n",
    "            # The raw prediction is a singleton list containing a float in (-1,1).\n",
    "            # We want only its contents:\n",
    "            prediction = network.predict(ex)[0]\n",
    "            # Categorize the prediction for accuracy comparison:\n",
    "            prediction = SUPERSET if prediction <= 0.0 else SUBSET            \n",
    "            predictions.append(prediction)\n",
    "            # Store the gold label for the classification report:\n",
    "            cats.append(cat[0])\n",
    "        # Report:\n",
    "        print \"======================================================================\"\n",
    "        print typ\n",
    "        print classification_report(cats, predictions, target_names=['SUPERSET', 'SUBSET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = build_dataset(wordentail_data, vector_func=randvec, vector_combo_func=vec_concatenate)\n",
    "\n",
    "network = ShallowNeuralNetwork(hidden_dim=40, maxiter=500, eta=0.05, display_progress=True)\n",
    "\n",
    "experiment(dataset, network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow neural network in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TensorFlowShallowNeuralNetwork:\n",
    "    def __init__(self, \n",
    "            input_dim=0, \n",
    "            hidden_dim=0, \n",
    "            output_dim=0,             \n",
    "            maxiter=100,\n",
    "            eta=0.05):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.maxiter = maxiter\n",
    "        self.eta = eta            \n",
    "                \n",
    "    def fit(self, training_data):\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        # Network initialization:\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.input_dim])\n",
    "        self.W1 = tf.Variable(tf.random_normal([self.input_dim, self.hidden_dim]))\n",
    "        self.b1 = tf.Variable(tf.random_normal([self.hidden_dim]))\n",
    "        self.W2 = tf.Variable(tf.random_normal([self.hidden_dim, self.output_dim]))\n",
    "        self.b2 = tf.Variable(tf.random_normal([self.output_dim]))\n",
    "        # Network structure:\n",
    "        self.h = tf.nn.tanh(tf.matmul(self.x, self.W1) + self.b1)\n",
    "        self.y = tf.nn.tanh(tf.matmul(self.h, self.W2) + self.b2)\n",
    "        self.y_ = tf.placeholder(tf.float32, [None, self.output_dim])\n",
    "        # Optimization:\n",
    "        mse = tf.reduce_sum(0.5 * (self.y_-self.y)**2)\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.eta).minimize(mse)\n",
    "        # Train:\n",
    "        init = tf.initialize_all_variables()\n",
    "        self.sess.run(init)        \n",
    "        x, y_ = zip(*training_data)\n",
    "        for iteration in range(self.maxiter):            \n",
    "            self.optimizer.run(feed_dict={self.x: x, self.y_: y_})                       \n",
    "\n",
    "    def predict(self, ex):\n",
    "         return self.sess.run(self.y, feed_dict={self.x: [ex]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = build_dataset(wordentail_data, vector_func=randvec, vector_combo_func=vec_concatenate)\n",
    "tfnet = TensorFlowShallowNeuralNetwork(hidden_dim=20, maxiter=1000)\n",
    "experiment(dataset, tfnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep neural network in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
