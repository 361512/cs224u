{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2016 term\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "0. [Overview](#Overview)\n",
    "0. [Set-up](#Set-up)\n",
    "0. [Working with SNLI](#Working-with-SNLI)\n",
    "   0. [Trees](#Trees)\n",
    "   0. [Readers](#Readers)\n",
    "0. [MaxEnt classifier approach](#MaxEnt-classifier-approach)\n",
    "   0. [Baseline classifier features](#Baseline-classifier-features)\n",
    "   0. [Building datasets for experiments](#Building-datasets-for-experiments)\n",
    "   0. [Training](#Training)\n",
    "   0. [Running experiments](#Running-experiments)\n",
    "0. [Recurrent neural network approach](#Shallow-neural-network-approach)\n",
    "0. [Additional NLI resources](#Additional-NLI-resources)\n",
    "0. [Homework 4](#Homework-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In the context of NLP/NLU, Natural Language Inference (NLI) is the task of predicting the logical relationships between words, phrases, sentences, (paragraphs, documents, ...). Such relationships are crucial for all kinds of reasoning in natural language: arguing, debating, problem solving, summarization, extrapolation, and so forth. \n",
    "\n",
    "NLI is a great task for this course. It requires serious linguistic analysis to do well, there are good publicly available datasets, and there are some natural baselines that help with getting a model up and running, and with understanding the performance of more sophisticated approaches.  NLI was also the topic of [Bill's thesis](http://nlp.stanford.edu/~wcmac/papers/nli-diss.pdf) (he popularized the name \"NLI\"), so you can forever endear yourself to him by working on it!\n",
    "\n",
    "We looked at NLI briefly in our word-level entailment bake-off (the `wordentail.ipynb` notebook). The purpose of this codebook is to introduce the problem of NLI more fully in the context of the [Stanford Natural Language Inference](http://nlp.stanford.edu/projects/snli/) corpus (SNLI). We'll explore two general approaches:\n",
    "\n",
    "* Standard classifiers\n",
    "* Recurrent neural networks\n",
    "\n",
    "This should be a good starting point for exploring richer models of NLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import codecs\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Make sure your environment includes all the requirements for [the cs224u repository](https://github.com/cgpotts/cs224u), especially TensorFlow, which isn't included in the standard Anaconda distribution (but is [easily installed](https://anaconda.org/jjhelmus/tensorflow)).\n",
    "0. Make sure `snli_sample_src` is pointing to your copy of `semparse_dateparse_data.pickle`, which should be included in the repository in the `nli-data` subfolder. (Because SNLI is very large, we'll work with a small sample from the training set in class.)\n",
    "0. For the homework: make sure you've run `nltk.download()` to get the NLTK data. (In particular, you need to use NLTL's WordNet API.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'dev']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Home for our SNLI sample:\n",
    "snli_sample_src = os.path.join('nli-data', 'snli_1.0_cs224u_sample.pickle')\n",
    "\n",
    "# Load the dataset: a dict with keys `train` and `dev`, where the values for\n",
    "# each are `dict`s sampled from the SNLI JSONL files.\n",
    "snli_sample = pickle.load(file(snli_sample_src))\n",
    "\n",
    "snli_sample.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with SNLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNLI contains both regular string representations of the data and unlabeled binary parses like the following:\n",
    "\n",
    "`\n",
    "entailment  ( ( A child ) ( is ( playing ( in ( a yard ) ) ) ) )  ( ( A child ) ( is playing ) )\n",
    "`\n",
    "\n",
    "`\n",
    "neutral  ( ( A child ) ( is ( playing ( in ( a yard ) ) ) ) )  ( ( A child ) ( is ( wearing ( blue jeans ) ) ) )\n",
    "`\n",
    "\n",
    "`\n",
    "contradiction  ( ( A child ) ( is ( playing ) ) )  ( ( A child ) ( is sleeping ) )\n",
    "`\n",
    "\n",
    "The brackets encode a label-free constituency structure of each sentence. The three labels on the left  are the classes that we want to learn to predict. We'll frequently need access to them, so let's define\n",
    "them as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABELS = ['contradiction', 'entailment', 'neutral']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set for SNLI contains 550,152 sentence pairs, with sentences varying in length from 2 to 62 words. This is too large for in-class experiments. This is why we're working with the sample in `snli_sample`:\n",
    "\n",
    "* `snli_sample['train']`: 12K sentence pairs\n",
    "* `snli_sample['test']`: 3K sentence pairs\n",
    "\n",
    "Both parts are balanced across the three classes, with sentences varying in length from 4 to 6 words. These limitations will allow us to explore lots of different models in class. You're encouraged to try out your ideas on the full dataset outside of class (perhaps as part of your final project)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function can be used to turn bracketed strings like the above into tuples of tuples encoding the syntactic structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WORD_RE = re.compile(r\"([^ \\(\\)]+)\", re.UNICODE)\n",
    "\n",
    "def str2tree(s):\n",
    "    \"\"\"Turns labeled bracketing s into a tree structure (tuple of tuples)\"\"\"\n",
    "    s = WORD_RE.sub(r'\"\\1\",', s)\n",
    "    s = s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "    s = s.replace(\")\", \"),\").strip(\",\")\n",
    "    s = s.strip(\",\")\n",
    "    return eval(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For baseline models, we often want just the words, also called terminal nodes or _leaves_.\n",
    "This function gives us access to them as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('A', 'child'), ('is', ('playing', ('in', ('a', 'yard')))))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str2tree(\"( ( A child ) ( is ( playing ( in ( a yard ) ) ) ) )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaves(t):\n",
    "    \"\"\"Returns all of the words (terminal nodes) in tree t\"\"\"\n",
    "    words = []\n",
    "    for x in t:\n",
    "        if isinstance(x, basestring):\n",
    "            words.append(x)\n",
    "        else:\n",
    "            words += leaves(x)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'child', 'is', 'playing', 'in', 'a', 'yard']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaves(str2tree(\"( ( A child ) ( is ( playing ( in ( a yard ) ) ) ) )\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easy to run through the corpus, let's define general readers for the data. The general function for this yields triples consisting of the the left tree and the right tree, as parsed by `str2tree`, and finally the label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def snli_reader(sample):\n",
    "    ######################################################################\n",
    "    \"\"\"Reader for SNLI data. `sample` just needs to be an iterator over\n",
    "    the SNLI JSONL files. For this notebook, it will always be \n",
    "    `snli_sample`, but, for example, the following should work for the \n",
    "    corpus files:\n",
    "    \n",
    "    import json    \n",
    "    def sample(src_filename):\n",
    "        for line in open(src_filename):\n",
    "            yield json.loads(line)\n",
    "    \n",
    "    Yields\n",
    "    ------\n",
    "    tuple\n",
    "        (tree1, tree1, label), where the trees are from `str2tree` and\n",
    "        label is in `LABELS` above.\n",
    "      \n",
    "    \"\"\"\n",
    "    for d in sample:\n",
    "        yield (str2tree(d['sentence1_binary_parse']), \n",
    "               str2tree(d['sentence2_binary_parse']),\n",
    "               d['gold_label'])\n",
    "        \n",
    "def train_reader():\n",
    "    \"\"\"Convenience function for reading just the training data.\"\"\"\n",
    "    return snli_reader(snli_sample['train'])\n",
    "\n",
    "def dev_reader():\n",
    "    \"\"\"Convenience function for reading just the dev data.\"\"\"\n",
    "    return snli_reader(snli_sample['dev'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxEnt classifier approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we'll adopt an approach that is essentially identical to that of the [supervisedsentiment.ipynb](supervisedsentiment.ipynb) notebook: we'll train simple MaxEnt classifiers on representations of the data obtained from hand-built feature functions. \n",
    "\n",
    "This notebook defines some common baseline features based on pairings of information in the premise and hypothesis. As usual, one can realize big performance gains quickly by improving on these baseline representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline classifier features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first baseline we define is the _word overlap_ baseline. It simply uses as\n",
    "features the words that appear in both sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_overlap_phi(t1, t2):    \n",
    "    \"\"\"Basis for features for the words in both the premise and hypothesis.\n",
    "    This tends to produce very sparse representations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t1, t2 : tuples\n",
    "        Tuples of tuples representing strings, as given by `str2tree`.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    defaultdict\n",
    "       Maps each word in both `t1` and `t2` to 1.\n",
    "    \n",
    "    \"\"\"\n",
    "    overlap = set([w1 for w1 in leaves(t1) if w1 in leaves(t2)])\n",
    "    return Counter(overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular baseline is to use as features the full cross-product of\n",
    "words from both sentences:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_cross_product_phi(t1, t2):\n",
    "    \"\"\"Basis for cross-product features. This tends to produce pretty \n",
    "    dense representations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t1, t2 : tuples\n",
    "        Tuples of tuples representing strings, as given by `str2tree`.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    defaultdict\n",
    "        Maps each (w1, w2) in the cross-product of `leaves(t1)` and \n",
    "        `leaves(t2)` to its count. This is a mult-set cross-product\n",
    "        (repeats matter).\n",
    "    \n",
    "    \"\"\"\n",
    "    return Counter([(w1, w2) for w1, w2 in itertools.product(leaves(t1), leaves(t2))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these feature functions return count dictionaries mapping feature names to  the number of times they occur in the data. This is the representation we'll work with throughout; `sklearn` will handle the further processing it needs to build linear classifiers.\n",
    "\n",
    "Naturally, you can do better than these feature functions! Both of these feature classes might be useful even in a more advanced model, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building datasets for experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, the first step in training a classifier is using a feature function like the one above to turn the data into a list of training instances (feature representations and their associated labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_dataset(reader, phi=word_overlap_phi, vectorizer=None):\n",
    "    \"\"\"Create a dataset for training classifiers using `sklearn`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    reader\n",
    "        An SNLI iterator like `snli_reader` above. Just needs to\n",
    "        yield (tree, tree, label) triples.\n",
    "        \n",
    "    phi : featuer function\n",
    "        Maps trees to count dictionaries.\n",
    "        \n",
    "    vectorizer : sklearn.feature_extraction.DictVectorizer    \n",
    "        If this is None, then a new `DictVectorizer` is created and\n",
    "        used to turn the list of dicts created by `phi` into a \n",
    "        feature matrix. This happens when we are training.\n",
    "              \n",
    "        If this is not None, then it's assumed to be a `DictVectorizer` \n",
    "        and used to transform the list of dicts. This happens in \n",
    "        assessment, when we take in new instances and need to \n",
    "        featurize them as we did in training.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dict with keys 'X' (the feature matrix), 'y' (the list of\n",
    "        labels), 'vectorizer' (the `DictVectorizer`), and \n",
    "        'raw_examples' (the original tree pairs, for error analysis).\n",
    "    \n",
    "    \"\"\"\n",
    "    feat_dicts = []\n",
    "    labels = []\n",
    "    raw_examples = []\n",
    "    for t1, t2, label in reader():\n",
    "        d = phi(t1, t2)\n",
    "        feat_dicts.append(d)\n",
    "        labels.append(label)   \n",
    "        raw_examples.append((t1, t2))\n",
    "    if vectorizer == None:\n",
    "        vectorizer = DictVectorizer(sparse=True)\n",
    "        feat_matrix = vectorizer.fit_transform(feat_dicts)\n",
    "    else:\n",
    "        feat_matrix = vectorizer.transform(feat_dicts)\n",
    "    return {'X': feat_matrix, \n",
    "            'y': labels, \n",
    "            'vectorizer': vectorizer, \n",
    "            'raw_examples': raw_examples}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep this notebook relatively simple, we adopt a bare-bones training framework, using just a standard-issue MaxEnt classifier. The following function is from [supervisedsentiment.ipynb](supervisedsentiment.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_maxent_classifier(X, y):    \n",
    "    \"\"\"Wrapper for `sklearn.linear.model.LogisticRegression`. This is also \n",
    "    called a Maximum Entropy (MaxEnt) Classifier, which is more fitting \n",
    "    for the multiclass case.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2d np.array\n",
    "        The matrix of features, one example per row.\n",
    "        \n",
    "    y : list\n",
    "        The list of labels for rows in `X`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sklearn.linear.model.LogisticRegression\n",
    "        A trained `LogisticRegression` instance.\n",
    "    \n",
    "    \"\"\"\n",
    "    mod = LogisticRegression(fit_intercept=True)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more robust and responsible approach, see [supervisedsentiment.ipynb](supervisedsentiment.ipynb) notebook, especially the [section on hyperparameter search](supervisedsentiment.ipynb#Hyperparameter-search). The [first homework problem below](#Homework-4) pushes you a bit in this direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Running experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `experiment` function handles the book-keeping associated with running experiments. It essentially just combines all of the above pieces in a flexible way. If you decide to expand this codebase for real experiments, then you'll likely want to incorporate more of the functionality from the [supervisedsentiment.ipynb](supervisedsentiment.ipynb) notebook, especially [its method for comparing different models statistically](supervisedsentiment.ipynb#Statistical-comparison-of-classifier-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def experiment(\n",
    "        train_reader=train_reader, \n",
    "        assess_reader=dev_reader, \n",
    "        phi=word_overlap_phi,\n",
    "        train_func=fit_maxent_classifier):  \n",
    "    \"\"\"Runs experiments on our SNLI fragment.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_reader, assess_reader\n",
    "        SNLI iterators like `snli_reader` above. Just needs to\n",
    "        yield (tree, tree, label) triples.\n",
    "        \n",
    "    phi : feature function (default: `word_overlap_phi`)\n",
    "        Maps trees to count dictionaries.\n",
    "        \n",
    "    train_func : model wrapper (default: `fit_maxent_classifier`)\n",
    "        Any function that takes a feature matrix and a label list\n",
    "        as its values and returns a fitted model with a `predict`\n",
    "        function that operates on feature matrices.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "         A formatted `classification_report` from `sklearn`.\n",
    "        \n",
    "    \"\"\"\n",
    "    train = build_dataset(train_reader, phi)    \n",
    "    assess = build_dataset(assess_reader, phi, vectorizer=train['vectorizer'])\n",
    "    mod = fit_maxent_classifier(train['X'], train['y'])\n",
    "    predictions = mod.predict(assess['X'])\n",
    "    return classification_report(assess['y'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.38      0.54      0.45      1000\n",
      " entailment       0.44      0.35      0.39      1000\n",
      "    neutral       0.37      0.29      0.32      1000\n",
      "\n",
      "avg / total       0.40      0.39      0.39      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few ideas for better classifier features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cross product of synsets compatible with each word, as given by WordNet. (Here is [a codebook on using WordNet from NLTK to do things like this](http://compprag.christopherpotts.net/wordnet.html).)\n",
    "\n",
    "* More fine-grained WordNet features &mdash; e.g., spotting pairs like _puppy_/_dog_ across the two sentences.\n",
    "\n",
    "* Use of other WordNet relations (see Table 1 and Table 2 in [this codelab](http://compprag.christopherpotts.net/wordnet.html) for relations and their coverage).\n",
    "\n",
    "* Using the tree structure to define features that are sensitive to how negation scopes over constituents.\n",
    "\n",
    "* Features that are sensitive to differences in negation between the two sentences.\n",
    "\n",
    "* Sentiment features seeking to identify contrasting polarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Recurrent neural network approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional NLI resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The folder [nli-data](nli-data) in this repository contains the NLI data from the [SemEval 2014 semantic relatedness task](http://alt.qcri.org/semeval2014/task1/). \n",
    "This data set is called \"Sentences Involving Compositional Knowledge\" or, for better or worse, \n",
    "\"SICK\". It's [freely available from the SemEval site](http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools).  [nli-data](nli-data) contains a parsed version created by [Sam Bowman](http://stanford.edu/~sbowman/) \n",
    "as part of [his research on neural models of semantic composition](https://github.com/sleepinyourhat/vector-entailment/releases/tag/W15-R1). \n",
    "\n",
    "* [SemEval 2013](https://www.cs.york.ac.uk/semeval-2013/) also had a wide range of interesting data sets for NLI and related tasks.\n",
    "\n",
    "* The [FraCaS textual inference test suite](http://www-nlp.stanford.edu/~wcmac/downloads/) is a smaller, hand-built dataset that is great for evaluating a model's ability to handle complex logical patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Feature selection [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a modification of `fit_maxent_classifier` called `fit_maxent_classifier_with_feature_selection` that does feature selection prior to fitting the model using  [sklearn.feature_selection.SelectPercentile](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectPercentile) with [sklearn.feature_selection.chi2](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2) as `score_func`. You can leave the default `percentile=10`. __Submit__:\n",
    "\n",
    "* Your `fit_maxent_classifier_with_feature_selection`.\n",
    "* Your use of `experiment` with `fit_maxent_classifier_with_feature_selection`.\n",
    "* Your output from the `experiment` function call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. WordNet-based entailment features [4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python NLTK](http://www.nltk.org) has an excellent WordNet interface. As noted above, WordNet is a natural choice for defining useful features in the context of NLI.\n",
    "\n",
    "__Your task__: write and submit a feature function, for use with `build_dataset`, that is just like `word_cross_product_phi` except that, given a sentence pair $(S_{1}, S_{2})$, it counts only pairs $(w_{1}, w_{2})$ such that $w_{1}$ entails $w_{2}$, for $w_{1} \\in S_{1}$ and $w_{2} \\in S_{2}$. For example, the sentence pair (_the cat runs_, _the animal moves_) would create the dictionary `{(cat, animal): 1.0, (runs, moves): 1.0}`.\n",
    "\n",
    "There are many ways to do this. For the purposes of the question, we can limit attention to the WordNet hypernym relation. The following illustrates reasonable ways to go from a string $s$ to the set of all hypernyms of Synsets consistent with $s$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('dog.n.01'), Synset('pup.n.01'), Synset('young_person.n.01')]\n",
      "[Synset('dog.n.01'), Synset('pup.n.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "    \n",
    "puppies = wn.synsets('puppy')\n",
    "print [h for ss in puppies for h in ss.hypernyms()]\n",
    "\n",
    "# A more conservative approach uses just the first-listed \n",
    "# Synset, which should be the most frequent sense:\n",
    "print wn.synsets('puppy')[0].hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. RNN [4 points]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
