{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2016 term\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "0. [Overview](#Overview)\n",
    "0. [Set-up](#Set-up)\n",
    "0. [Working with SNLI](#Working-with-SNLI)\n",
    "   0. [Trees](#Trees)\n",
    "   0. [Readers](#Readers)\n",
    "0. [MaxEnt classifier approach](#MaxEnt-classifier-approach)\n",
    "   0. [Baseline classifier features](#Baseline-classifier-features)\n",
    "   0. [Building datasets for experiments](#Building-datasets-for-experiments)\n",
    "   0. [Training](#Training)\n",
    "   0. [Running experiments](#Running-experiments)\n",
    "0. [Recurrent neural network approach](#Shallow-neural-network-approach)\n",
    "0. [Exercises](#Exercises)\n",
    "    0. [Feature selection](#A.-Feature-selection)\n",
    "    0. [WordNet-based entailment features](#B.-WordNet-based-entailment-features)\n",
    "    0. [A feed-forward neural baseline](#C.-A-neural-baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In the context of NLP/NLU, Natural Language Inference (NLI) is the task of predicting the logical relationships between words, phrases, sentences, (paragraphs, documents, ...). Such relationships are crucial for all kinds of reasoning in natural language: arguing, debating, problem solving, summarization, extrapolation, and so forth. \n",
    "\n",
    "NLI is a great task for this course. It requires serious linguistic analysis to do well, there are good publicly available datasets, and there are some natural baselines that help with getting a model up and running, and with understanding the performance of more sophisticated approaches.  NLI was also the topic of [Bill's thesis](http://nlp.stanford.edu/~wcmac/papers/nli-diss.pdf) (he popularized the name \"NLI\"), so you can forever endear yourself to him by working on it!\n",
    "\n",
    "We looked at NLI briefly in our word-level entailment bake-off (the `wordentail.ipynb` notebook). The purpose of this codebook is to introduce the problem of NLI more fully in the context of the [Stanford Natural Language Inference](http://nlp.stanford.edu/projects/snli/) corpus (SNLI). We'll explore two general approaches:\n",
    "\n",
    "* Standard classifiers\n",
    "* Recurrent neural networks\n",
    "\n",
    "This should be a good starting point for exploring richer models of NLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import codecs\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Make sure your environment includes all the requirements for [the cs224u repository](https://github.com/cgpotts/cs224u), especially TensorFlow, which isn't included in the standard Anaconda distribution (but is [easily installed](https://anaconda.org/jjhelmus/tensorflow)).\n",
    "0. Make sure `snli_sample_src` is pointing to your copy of `semparse_dateparse_data.pickle`, which should be included in the repository in the `nli-data` subfolder. (Because SNLI is very large, we'll work with a small sample from the training set in class.)\n",
    "0. For exercise B in section 6, make sure you've run `nltk.download()` to get the NLTK data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'dev']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snli_sample_src = os.path.join('nli-data', 'snli_1.0_cs224u_sample.pickle')\n",
    "\n",
    "snli_sample = pickle.load(file(snli_sample_src))\n",
    "\n",
    "snli_sample.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with SNLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WORD_RE = re.compile(r\"([^ \\(\\)]+)\", re.UNICODE)\n",
    "\n",
    "def str2tree(s):\n",
    "    \"\"\"Turns labeled bracketing s into a tree structure (tuple of tuples)\"\"\"\n",
    "    s = WORD_RE.sub(r'\"\\1\",', s)\n",
    "    s = s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "    s = s.replace(\")\", \"),\").strip(\",\")\n",
    "    s = s.strip(\",\")\n",
    "    return eval(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For baseline models, we often want just the words, also called terminal nodes or _leaves_.\n",
    "This function gives us access to them as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('A', 'child'), ('is', ('playing', ('in', ('a', 'yard')))))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str2tree(\"( ( A child ) ( is ( playing ( in ( a yard ) ) ) ) )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaves(t):\n",
    "    \"\"\"Returns all of the words (terminal nodes) in tree t\"\"\"\n",
    "    words = []\n",
    "    for x in t:\n",
    "        if isinstance(x, basestring):\n",
    "            words.append(x)\n",
    "        else:\n",
    "            words += leaves(x)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'child', 'is', 'playing', 'in', 'a', 'yard']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaves(str2tree(\"( ( A child ) ( is ( playing ( in ( a yard ) ) ) ) )\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABELS = ['contradiction', 'entailment', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def snli_reader(sample):\n",
    "    for d in sample:\n",
    "        yield (str2tree(d['sentence1_binary_parse']), \n",
    "               str2tree(d['sentence2_binary_parse']),\n",
    "               d['gold_label'])\n",
    "        \n",
    "def train_reader():\n",
    "    return snli_reader(snli_sample['train'])\n",
    "\n",
    "def dev_reader():\n",
    "    return snli_reader(snli_sample['dev'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxEnt classifier approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline classifier features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first baseline we define is the _word overlap_ baseline. It simply uses as\n",
    "features the words that appear in both sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_overlap_phi(t1, t2):\n",
    "    overlap = [w1 for w1 in leaves(t1) if w1 in leaves(t2)]\n",
    "    return Counter(overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular baseline is to use as features the full cross-product of\n",
    "words from both sentences:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_cross_product_phi(t1, t2):\n",
    "    return Counter([(w1, w2) for w1, w2 in itertools.product(leaves(t1), leaves(t2))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these feature functions return count dictionaries mapping feature names to  the number of times they occur in the data. This is the representation we'll work with throughout; `sklearn` will handle the further processing it needs to build linear classifiers.\n",
    "\n",
    "Naturally, you can do better than these feature functions! Both of these feature classes might be useful even in a more advanced model, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building datasets for experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in training a classifier is using a feature function like the one above to turn the data into a list of _training instances_: feature representations and their  associated labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_dataset(reader, phi=word_overlap_phi, vectorizer=None):\n",
    "    feat_dicts = []\n",
    "    labels = []\n",
    "    raw_examples = []\n",
    "    for t1, t2, label in reader():\n",
    "        d = phi(t1, t2)\n",
    "        feat_dicts.append(d)\n",
    "        labels.append(label)   \n",
    "        raw_examples.append((t1, t2))\n",
    "    if vectorizer == None:\n",
    "        vectorizer = DictVectorizer(sparse=True)\n",
    "        feat_matrix = vectorizer.fit_transform(feat_dicts)\n",
    "    else:\n",
    "        feat_matrix = vectorizer.transform(feat_dicts)\n",
    "    return {'X': feat_matrix, \n",
    "            'y': labels, \n",
    "            'vectorizer': vectorizer, \n",
    "            'raw_examples': raw_examples}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_maxent_classifier(X, y):\n",
    "    mod = LogisticRegression(fit_intercept=True, C=1.0, penalty='l2')\n",
    "    mod.fit(X, y)\n",
    "    return mod    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def experiment(\n",
    "        train_reader=train_reader, \n",
    "        assess_reader=dev_reader, \n",
    "        phi=word_overlap_phi,\n",
    "        train_func=fit_maxent_classifier):    \n",
    "    train = build_dataset(train_reader, phi)    \n",
    "    assess = build_dataset(assess_reader, phi, vectorizer=train['vectorizer'])\n",
    "    mod = fit_maxent_classifier(train['X'], train['y'])\n",
    "    predictions = mod.predict(assess['X'])\n",
    "    return classification_report(assess['y'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.38      0.54      0.45      1000\n",
      " entailment       0.44      0.35      0.39      1000\n",
      "    neutral       0.37      0.29      0.32      1000\n",
      "\n",
      "avg / total       0.40      0.39      0.39      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few ideas for better classifier features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cross product of synsets compatible with each word, as given by WordNet. (Here is [a codebook on using WordNet from NLTK to do things like this](http://compprag.christopherpotts.net/wordnet.html).)\n",
    "\n",
    "* More fine-grained WordNet features &mdash; e.g., spotting pairs like _puppy_/_dog_ across the two sentences.\n",
    "\n",
    "* Use of other WordNet relations (see Table 1 and Table 2 in [this codelab](http://compprag.christopherpotts.net/wordnet.html) for relations and their coverage).\n",
    "\n",
    "* Using the tree structure to define features that are sensitive to how negation scopes over constituents.\n",
    "\n",
    "* Features that are sensitive to differences in negation between the two sentences.\n",
    "\n",
    "* Sentiment features seeking to identify contrasting polarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Recurrent neural network approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Feature selection\n",
    "\n",
    "Create a modification of `fit_maxent_classifier` called `fit_maxent_classifier_with_feature_selection` that does feature selection prior to fitting the model using  [sklearn.feature_selection.SelectPercentile](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectPercentile) with [sklearn.feature_selection.chi2](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2) as `score_func`. You can leave the default `percentile=10`. __Submit__:\n",
    "\n",
    "* Your `fit_maxent_classifier_with_feature_selection`.\n",
    "* Your use of `experiment` with `fit_maxent_classifier_with_feature_selection`.\n",
    "* Your output from the `experiment` function call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. WordNet-based entailment features\n",
    "\n",
    "[Python NLTK](http://www.nltk.org) has an excellent WordNet interface. As noted above, WordNet is a natural choice for defining useful features in the context of NLI.\n",
    "\n",
    "__Your task__: write and submit a feature function, for use with `build_dataset`, that is just like `word_cross_product_phi` except that, given a sentence pair $(S_{1}, S_{2})$, it counts only pairs $(w_{1}, w_{2})$ such that $w_{1}$ entails $w_{2}$, for $w_{1} \\in S_{1}$ and $w_{2} \\in S_{2}$. For example, the sentence pair (_the cat runs_, _the animal moves_) would create the dictionary `{(cat, animal): 1.0, (runs, moves): 1.0}`.\n",
    "\n",
    "There are many ways to do this. For the purposes of the question, we can limit attention to the WordNet hypernym relation. The following illustrates reasonable ways to go from a string $s$ to the set of all hypernyms of Synsets consistent with $s$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('dog.n.01'), Synset('pup.n.01'), Synset('young_person.n.01')]\n",
      "[Synset('dog.n.01'), Synset('pup.n.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "    \n",
    "puppies = wn.synsets('puppy')\n",
    "print [h for ss in puppies for h in ss.hypernyms()]\n",
    "\n",
    "# A more conservative approach uses just the first-listed \n",
    "# Synset, which should be the most frequent sense:\n",
    "print wn.synsets('puppy')[0].hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. A neural baseline\n",
    "\n",
    "This question asked you to define and evaluate a GloVe-based vector-average neural baseline for SNLI. \n",
    "\n",
    "__Submit__: \n",
    "\n",
    "* A feature function `glove_average_concatenate_phi` that averages the GloVe representations for the words in premise, averages the Glove representation of the words in the hypothesis, and concatenates those two vectors.\n",
    "\n",
    "* An experiment function `shallow_snli_experiment` that uses `glove_average_concatenate_phi` to train and assses a `TfShallowNeuralNetwork`.\n",
    "\n",
    "Note: in this case, your network should have an output layer of dimension 3, one for each SNLI class. Your prediction can be the argmax of this predicted output layer: `LABELS[np.argmax(prediction)]`, where `prediction` is the output of `TfShallowNeuralNetwork.predict`. \n",
    "\n",
    "The code below is intended to provide guidance as to how to structure this answer. Please feel free to use a different design if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_home = 'glove.6B'\n",
    "GLOVE = utils.glove2dict(os.path.join(glove_home, 'glove.6B.50d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def glove_features(t):\n",
    "    \"\"\"Return the mean glove vector of the leaves in tree t.\"\"\"\n",
    "    vecs = [GLOVE[w] for w in leaves(t) if w in GLOVE]\n",
    "    if vecs:\n",
    "        return np.mean(vecs, axis=0)\n",
    "    else:\n",
    "        return randvec('x', 50)\n",
    "    \n",
    "def vec_concatenate(u, v):\n",
    "    return np.concatenate((u, v)) \n",
    "\n",
    "def randvec(w, n=40, lower=-0.5, upper=0.5):\n",
    "    return np.array([random.uniform(lower, upper) for i in range(n)])\n",
    "\n",
    "def glove_featurizer(t1, t2, dim=50):\n",
    "    return vec_concatenate(glove_features(t1), glove_features(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labelvec(label):\n",
    "    \"\"\"Return output vectors like [1,-1,-1], where the unique 1 is the true label.\"\"\"\n",
    "    vec = np.repeat(-1.0, 3)\n",
    "    vec[LABELS.index(label)] = 1.0\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_prep(reader, phi):         \n",
    "    dataset = []\n",
    "    for t1, t2, label in reader():     \n",
    "        dataset.append([phi(t1, t2), labelvec(label)])\n",
    "    return dataset\n",
    "\n",
    "def train_and_evaluate_network(network, phi, train_reader=dev_reader, assess_reader=dev_reader):\n",
    "    # Use `data_prep` to prepare the train and assess data with \n",
    "    # their readers and `phi`.\n",
    "    train = data_prep(train_reader, phi)\n",
    "    assess = data_prep(assess_reader, phi)\n",
    "    # Train `network` using its `fit` method:\n",
    "    network.fit(train)\n",
    "    # Store predictions and gold labels:\n",
    "    predictions = []\n",
    "    gold = []\n",
    "    # Iterate through the assessment data:\n",
    "    for ex, cat in assess:            \n",
    "        # Use `network.predict` to get the prediction for `ex`:\n",
    "        prediction = network.predict(ex)\n",
    "        # Argmax dimension for the prediction:\n",
    "        prediction = LABELS[np.argmax(prediction)]\n",
    "        predictions.append(prediction)\n",
    "        # Store the gold label for the classification report:\n",
    "        gold.append(LABELS[np.argmax(cat)])        \n",
    "    # Report:\n",
    "    return classification_report(gold, predictions, target_names=LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "completed iteration 100; error is 3781.86294107"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.44      0.58      0.50      1000\n",
      "   entailment       0.56      0.43      0.48      1000\n",
      "      neutral       0.49      0.46      0.47      1000\n",
      "\n",
      "  avg / total       0.50      0.49      0.49      3000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import shallow_neural_networks\n",
    "\n",
    "print train_and_evaluate_network(\n",
    "    shallow_neural_networks.ShallowNeuralNetwork(hidden_dim=5, maxiter=100), \n",
    "    glove_featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.33      1.00      0.50      1000\n",
      "   entailment       0.00      0.00      0.00      1000\n",
      "      neutral       0.00      0.00      0.00      1000\n",
      "\n",
      "  avg / total       0.11      0.33      0.17      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import shallow_neural_networks\n",
    "\n",
    "print train_and_evaluate_network(\n",
    "    shallow_neural_networks.TfShallowNeuralNetwork(), \n",
    "    glove_featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
