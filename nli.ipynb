{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2016 term\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "0. [Overview](#Overview)\n",
    "0. [Set-up](#Set-up)\n",
    "0. [Working with SNLI](#Working-with-SNLI)\n",
    "   0. [Trees](#Trees)\n",
    "   0. [Readers](#Readers)\n",
    "0. [Linear classifier approach](#Linear-classifier-approach)\n",
    "   0. [Baseline linear classifier features](#Baseline-linear-classifier-features)\n",
    "   0. [Building datasets for linear classifier experiments](#Building-datasets-for-linear-classifier-experiments)\n",
    "   0. [Training linear classifiers](#Training-linear-classifiers)\n",
    "   0. [Running linear classifier experiments](#Running-linear-classifier-experiments)\n",
    "0. [Recurrent neural network approach](#Recurrent-neural-network-approach)\n",
    "   0. [Classifier RNN model definition](#Classifier-RNN-model-definition)\n",
    "   0. [Building datasets for classifier RNNs](#Building-datasets-for-classifier-RNNs)\n",
    "   0. [Running classifier RNN experiments](#Running-classifier-RNN-experiments)\n",
    "   0. [Next steps for NLI deep learning models](#Next-steps-for-NLI-deep-learning-models)\n",
    "0. [Additional NLI resources](#Additional-NLI-resources)\n",
    "0. [Homework 4](#Homework-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Natural Language Inference (NLI) is the task of predicting the logical relationships between words, phrases, sentences, (paragraphs, documents, ...). Such relationships are crucial for all kinds of reasoning in natural language: arguing, debating, problem solving, summarization, and so forth. \n",
    "\n",
    "Our NLI data will look like this:\n",
    "\n",
    "* (_every dog danced_, _every puppy moved_) $\\Rightarrow$ __entailment__\n",
    "* (_a puppy danced_, _no dog moved_) $\\Rightarrow$ __contradiction__\n",
    "* (_a dog moved_, _no puppy danced_) $\\Rightarrow$ __neutral__\n",
    "\n",
    "The first sentence is the __premise__ and the second is the __hypothesis__ (logicians call it the __conclusion__).\n",
    "\n",
    "We looked at NLI briefly in our word-level entailment bake-off (the [wordentail.ipynb](wordentail.ipynb) notebook). The purpose of this codebook is to introduce the problem of NLI more fully in the context of the [Stanford Natural Language Inference](http://nlp.stanford.edu/projects/snli/) corpus (SNLI). We'll explore two general approaches:\n",
    "\n",
    "* Standard linear classifiers\n",
    "* Recurrent neural networks\n",
    "\n",
    "This should be a good starting point for exploring richer models of NLI. It's also fun because it sets up a battle royale between models that require serious linguistic analysis (the linear ones) and models that are claimed by advocates to require no such analysis (deep learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import utils\n",
    "from nltk.tree import Tree\n",
    "from nli_rnn import ClassifierRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats=['svg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Make sure your environment includes all the requirements for [the cs224u repository](https://github.com/cgpotts/cs224u).\n",
    "0. Make sure `snli_sample_src` (just below) is pointing to your copy of `snli_1.0_cs224u_sample.pickle`, which should be included in the repository in the `nli-data` subfolder. (Because SNLI is very large, we'll work with a small sample from the training set in class.)\n",
    "0. For the homework: make sure you've run `nltk.download()` to get the NLTK data. (In particular, you need to use NLTK's WordNet API.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vocab', u'train', u'dev']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Home for our SNLI sample:\n",
    "snli_sample_src = os.path.join('nli-data', 'snli_1.0_cs224u_sample.pickle')\n",
    "\n",
    "# Load the dataset: a dict with keys `train`, `dev`, and `vocab`. The first\n",
    "# two are lists of `dict`s sampled from the SNLI JSONL files. The third is\n",
    "# the complete vocabulary of the leaves in the trees for `train` and `dev`.\n",
    "snli_sample = pickle.load(open(snli_sample_src, 'rb'))\n",
    "\n",
    "snli_sample.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with SNLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNLI contains both regular string representations of the data and unlabeled binary parses like the following:\n",
    "\n",
    "`\n",
    "( ( A child ) ( is ( playing ( in ( a yard ) ) ) ) )\n",
    "`\n",
    "\n",
    "The brackets encode a label-free constituency structure of each sentence. \n",
    "\n",
    "Here are the class labels that we wish to learn to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABELS = ['contradiction', 'entailment', 'neutral']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set for SNLI contains 550,152 sentence pairs, with sentences varying in length from 2 to 62 words. This is too large for in-class experiments and assignments. This is why we're working with the sample in `snli_sample`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(snli_sample['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(snli_sample['dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6367"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(snli_sample['vocab'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `train` and `test` are balanced across the three classes, with sentences varying in length from 3 to 6 words. These limitations will allow us to explore lots of different models in class. You're encouraged to try out your ideas on the full dataset outside of class (perhaps as part of your final project)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function can be used to turn bracketed strings like the above into trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def str2tree(s):\n",
    "    \"\"\"Map str `s` to an `nltk.tree.Tree` instance. The assumption is that \n",
    "    `s` represents a tree with no node labels. We add a nonce label 'X'\n",
    "    for compatibility with `Tree.fromstring`.\"\"\"\n",
    "    s = s.replace(\"(\", \"(X\")\n",
    "    return Tree.fromstring(s)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('X', [Tree('X', ['A', 'child']), Tree('X', ['is', Tree('X', ['playing', Tree('X', ['in', Tree('X', ['a', 'yard'])])])])])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = str2tree(\"( ( A child ) ( is ( playing ( in ( a yard ) ) ) ) )\")\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For baseline models, we often want just the words, also called terminal nodes or _leaves_. We can access them with the `leaves` method on `nltk.tree.Tree` instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'child', 'is', 'playing', 'in', 'a', 'yard']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.leaves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easy to run through the corpus, let's define general readers for the data. The general function for this yields triples consisting of the the left tree and the right tree, as parsed by `str2tree`, and finally the label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def snli_reader(sample):\n",
    "    \"\"\"Reader for SNLI data. `sample` just needs to be an iterator over\n",
    "    the SNLI JSONL files. For this notebook, it will always be \n",
    "    `snli_sample`, but, for example, the following should work for the \n",
    "    corpus files:\n",
    "    \n",
    "    import json    \n",
    "    def sample(src_filename):\n",
    "        for line in open(src_filename):\n",
    "            yield json.loads(line)\n",
    "    \n",
    "    Yields\n",
    "    ------\n",
    "    tuple\n",
    "        (tree1, tree2, label), where the trees are from `str2tree` and\n",
    "        label is in `LABELS` above.\n",
    "      \n",
    "    \"\"\"\n",
    "    for d in sample:\n",
    "        yield (str2tree(d['sentence1_binary_parse']), \n",
    "               str2tree(d['sentence2_binary_parse']),\n",
    "               d['gold_label'])\n",
    "        \n",
    "def train_reader():\n",
    "    \"\"\"Convenience function for reading just the training data.\"\"\"\n",
    "    return snli_reader(snli_sample['train'])\n",
    "\n",
    "def dev_reader():\n",
    "    \"\"\"Convenience function for reading just the dev data.\"\"\"\n",
    "    return snli_reader(snli_sample['dev'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classifier approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we'll adopt an approach that is essentially identical to that of the [supervisedsentiment.ipynb](supervisedsentiment.ipynb) notebook: we'll train simple MaxEnt classifiers on representations of the data obtained from hand-built feature functions. \n",
    "\n",
    "This notebook defines some common baseline features based on pairings of information in the premise and hypothesis. As usual, one can realize big performance gains quickly by improving on these baseline representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline linear classifier features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first baseline we define is the _word overlap_ baseline. It simply uses as\n",
    "features the words that appear in both sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_overlap_phi(t1, t2):    \n",
    "    \"\"\"Basis for features for the words in both the premise and hypothesis.\n",
    "    This tends to produce very sparse representations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t1, t2 : `nltk.tree.Tree`\n",
    "        As given by `str2tree`.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    defaultdict\n",
    "       Maps each word in both `t1` and `t2` to 1.\n",
    "    \n",
    "    \"\"\"\n",
    "    overlap = set([w1 for w1 in t1.leaves() if w1 in t2.leaves()])\n",
    "    return Counter(overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular baseline is  the full cross-product of words from both sentences:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_cross_product_phi(t1, t2):\n",
    "    \"\"\"Basis for cross-product features. This tends to produce pretty \n",
    "    dense representations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t1, t2 : `nltk.tree.Tree`\n",
    "        As given by `str2tree`.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    defaultdict\n",
    "        Maps each (w1, w2) in the cross-product of `t1.leaves()` and \n",
    "        `t2.leaves()` to its count. This is a multi-set cross-product\n",
    "        (repetitions matter).\n",
    "    \n",
    "    \"\"\"\n",
    "    return Counter([(w1, w2) for w1, w2 in itertools.product(t1.leaves(), t2.leaves())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these feature functions return count dictionaries mapping feature names to  the number of times they occur in the data. This is the representation we'll work with throughout; `sklearn` will handle the further processing it needs to build linear classifiers.\n",
    "\n",
    "Naturally, you can do better than these feature functions! Both of these  might be useful even in a more advanced model, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building datasets for linear classifier experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, the first step in training a classifier is using a feature function like the one above to turn the data into a list of training instances (feature representations and their associated labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_linear_classifier_dataset(\n",
    "        reader, \n",
    "        phi=word_overlap_phi, \n",
    "        vectorizer=None):\n",
    "    \"\"\"Create a dataset for training classifiers using `sklearn`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    reader\n",
    "        An SNLI iterator like `snli_reader` above. Just needs to\n",
    "        yield (tree, tree, label) triples.\n",
    "        \n",
    "    phi : feature function\n",
    "        Maps trees to count dictionaries.\n",
    "        \n",
    "    vectorizer : `sklearn.feature_extraction.DictVectorizer`   \n",
    "        If this is None, then a new `DictVectorizer` is created and\n",
    "        used to turn the list of dicts created by `phi` into a \n",
    "        feature matrix. This happens when we are training.\n",
    "              \n",
    "        If this is not None, then it's assumed to be a `DictVectorizer` \n",
    "        and used to transform the list of dicts. This happens in \n",
    "        assessment, when we take in new instances and need to \n",
    "        featurize them as we did in training.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dict with keys 'X' (the feature matrix), 'y' (the list of\n",
    "        labels), 'vectorizer' (the `DictVectorizer`), and \n",
    "        'raw_examples' (the original tree pairs, for error analysis).\n",
    "    \n",
    "    \"\"\"\n",
    "    feat_dicts = []\n",
    "    labels = []\n",
    "    raw_examples = []\n",
    "    for t1, t2, label in reader():\n",
    "        d = phi(t1, t2)\n",
    "        feat_dicts.append(d)\n",
    "        labels.append(label)   \n",
    "        raw_examples.append((t1, t2))\n",
    "    if vectorizer == None:\n",
    "        vectorizer = DictVectorizer(sparse=True)\n",
    "        feat_matrix = vectorizer.fit_transform(feat_dicts)\n",
    "    else:\n",
    "        feat_matrix = vectorizer.transform(feat_dicts)\n",
    "    return {'X': feat_matrix, \n",
    "            'y': labels, \n",
    "            'vectorizer': vectorizer, \n",
    "            'raw_examples': raw_examples}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training linear classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep this notebook relatively simple, we adopt a bare-bones training framework, using just a standard-issue MaxEnt classifier. The following function is from [supervisedsentiment.ipynb](supervisedsentiment.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_maxent_classifier(X, y):    \n",
    "    \"\"\"Wrapper for `sklearn.linear.model.LogisticRegression`. This is also \n",
    "    called a Maximum Entropy (MaxEnt) Classifier, which is more fitting \n",
    "    for the multiclass case.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2d np.array\n",
    "        The matrix of features, one example per row.\n",
    "        \n",
    "    y : list\n",
    "        The list of labels for rows in `X`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    `sklearn.linear.model.LogisticRegression`\n",
    "        A trained `LogisticRegression` instance.\n",
    "    \n",
    "    \"\"\"\n",
    "    mod = LogisticRegression(fit_intercept=True)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more robust and responsible approach, see [supervisedsentiment.ipynb](supervisedsentiment.ipynb) notebook, especially the [section on hyperparameter search](supervisedsentiment.ipynb#Hyperparameter-search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Running linear classifier experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `linear_classifier_experiment` function handles the book-keeping associated with running experiments. It essentially just combines all of the above pieces in a flexible way. If you decide to expand this codebase for real experiments, then you'll likely want to incorporate more of the functionality from the [supervisedsentiment.ipynb](supervisedsentiment.ipynb) notebook, especially [its method for comparing different models statistically](supervisedsentiment.ipynb#Statistical-comparison-of-classifier-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_classifier_experiment(\n",
    "        train_reader=train_reader, \n",
    "        assess_reader=dev_reader, \n",
    "        phi=word_overlap_phi,\n",
    "        train_func=fit_maxent_classifier):  \n",
    "    \"\"\"Runs experiments on our SNLI fragment.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_reader, assess_reader\n",
    "        SNLI iterators like `snli_reader` above. Just needs to\n",
    "        yield (tree, tree, label) triples.\n",
    "        \n",
    "    phi : feature function (default: `word_overlap_phi`)\n",
    "        Maps trees to count dictionaries.\n",
    "        \n",
    "    train_func : model wrapper (default: `fit_maxent_classifier`)\n",
    "        Any function that takes a feature matrix and a label list\n",
    "        as its values and returns a fitted model with a `predict`\n",
    "        function that operates on feature matrices.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A formatted `classification_report` from `sklearn`.\n",
    "        \n",
    "    \"\"\"\n",
    "    train = build_linear_classifier_dataset(train_reader, phi)    \n",
    "    assess = build_linear_classifier_dataset(assess_reader, phi, vectorizer=train['vectorizer'])\n",
    "    mod = fit_maxent_classifier(train['X'], train['y'])\n",
    "    predictions = mod.predict(assess['X'])\n",
    "    return classification_report(assess['y'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.38      0.49      0.42       500\n",
      " entailment       0.43      0.34      0.38       500\n",
      "    neutral       0.32      0.30      0.31       500\n",
      "\n",
      "avg / total       0.38      0.38      0.37      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(linear_classifier_experiment())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.56      0.49      0.52       500\n",
      " entailment       0.48      0.55      0.52       500\n",
      "    neutral       0.44      0.43      0.44       500\n",
      "\n",
      "avg / total       0.49      0.49      0.49      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(linear_classifier_experiment(phi=word_cross_product_phi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few ideas for better classifier features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cross product of synsets compatible with each word, as given by WordNet. (Here is [a codebook on using WordNet from NLTK to do things like this](http://compprag.christopherpotts.net/wordnet.html).)\n",
    "\n",
    "* More fine-grained WordNet features &mdash; e.g., spotting pairs like _puppy_/_dog_ across the two sentences.\n",
    "\n",
    "* Use of other WordNet relations (see Table 1 and Table 2 in [this codelab](http://compprag.christopherpotts.net/wordnet.html) for relations and their coverage).\n",
    "\n",
    "* Using the tree structure to define features that are sensitive to how negation scopes over constituents.\n",
    "\n",
    "* Features that are sensitive to differences in negation between the two sentences.\n",
    "\n",
    "* Sentiment features seeking to identify contrasting sentiment polarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Recurrent neural network approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very recently, recurrent neural networks (RNNs) have become one of the dominant approaches to NLI, and there is a great deal of interest in the extent to which they can learn to simulate the powerful symbolic approaches that have long dominated work in NLI. \n",
    "\n",
    "The goal of this section is to give you some hands-on experience with using RNNs to build NLI models. Because these models are demanding not only in terms of data but also in terms of training time, we'll just get a glimpse of their potential, but I think even this glimpse clearly indicates their great potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier RNN model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we'll be exploring is probably the simplest one that fits the NLI problem. It's depicted in the following diagram:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig/nli-rnn.svg\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model would actually work for any classification task. For instance, you could revisit the [supervisedsentiment](supervisedsentiment.ipydb) notebook and try it out on the Stanford Sentiment Treebank.\n",
    "\n",
    "The dominant applications for RNNs to date have been for language modeling and machine translation. Those models have many more output vectors than ours. For a wonderful step-by-step introduction to such models, see Denny Britz's [four-part tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) (in the form of a notebook like this one). See also Andrej Karpathy's [insightful, clear overview of different RNN architectures](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). (Both Denny and Andrej are Stanford researchers!)\n",
    "\n",
    "The above diagram is a kind of schematic for the following model definition:\n",
    "\n",
    "$$h_{t} = \\tanh\\left(x_{t}W_{xh} + h_{t-1}W_{hh}\\right)$$\n",
    "\n",
    "$$y = \\text{softmax}\\left(h_{n}W_{hy}\\right)$$\n",
    "\n",
    "where $n$ is the sequence length and $1 \\leqslant t \\leqslant n$. As indicated in the above diagram, the sequence of hidden states is padded with an initial state $h_{0}$. In our implementation, this is always an all $0$ vector, but it can be initialized in more sophisticated ways.\n",
    "\n",
    "It's important to see that there is just one $W_{xh}$, just one $W_{hh}$, and just one $W_{hy}$. \n",
    "\n",
    "Our from-scratch implementation of the above model is in [nli_rnn.py](nli_rnn.py). As usual, the goal of this code is to illuminate the above concepts and clear up any lingering underspecification in descriptions like the above. The code also shows how __backpropagation through time__ works in these models. You'll see that it is very similar to regular backpropagation as we used it in the simpler [word-entailment bake-off](wordentail.ipynb) (using the feed-forward networks from [shallow_neural_networks.py](shallow_neural_networks.py).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building datasets for classifier RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function uses our `snli_reader` infrastructure to create datasets for training and assessing RNNs. The steps:\n",
    "\n",
    "* Concatenate the leaves of the premise and hypothesis trees into a sequence\n",
    "* Use the `LABELS` vector defined above to turn each string label into a one-hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_rnn_dataset(reader):\n",
    "    \"\"\"Build RNN datasets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    reader\n",
    "        SNLI iterator like `snli_reader` above. Just needs to\n",
    "        yield (tree, tree, label) triples.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list of tuples\n",
    "        The first member of each tuple is a list of strings (the\n",
    "        concatenated leaves) and the second is an np.array \n",
    "        (dimension 3) with a single 1 for the true class and 0s\n",
    "        in the other two positions\n",
    "       \n",
    "    \"\"\"    \n",
    "    dataset = []\n",
    "    for (t1, t2, label) in reader():\n",
    "        seq = t1.leaves() + t2.leaves()\n",
    "        y_ = np.zeros(3)\n",
    "        y_[LABELS.index(label)] = 1.0\n",
    "        dataset.append((seq, y_))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running classifier RNN experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nex we define functions for the training and assessment steps. It's currently baked in that you want to train with `train_reader` and assess on `dev_reader`. If you start doing serious experiments, you'll want to move to a more flexible set-up like the one we established above for linear classifiers (and see [supervisedsentiment.ipynb](supervisedsentiment.ipynb) for even more ideas).\n",
    "\n",
    "The important thing to see about this function is that it requires a `vocab` argument and an `embedding` argument:\n",
    "\n",
    "* `vocab` is a list of strings. It needs to contain every word we'll encounter in training or assessment.\n",
    "* `embedding` is a 2d matrix in which the ith row gives the input representation for the ith member of `vocab`.\n",
    "\n",
    "This gives you flexibility in how you represent the inputs. In the experiment run below, the inputs are just random vectors, but [the homework](#Homework-4) asks you to try out GloVe inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rnn_experiment(\n",
    "        vocab, \n",
    "        embedding, \n",
    "        hidden_dim=10, \n",
    "        eta=0.05, \n",
    "        maxiter=10):\n",
    "    \"\"\"Classifier RNN experiments.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab : list of str\n",
    "        Must contain every word we'll encounter in training or assessment.\n",
    "        \n",
    "    embedding : np.array\n",
    "        Embedding matrix for `vocab`. The ith row gives the input \n",
    "        representation for the ith member of vocab. Thus, `embedding`\n",
    "        must have the same row count as the length of vocab. Its\n",
    "        columns can be any length. (That is, the input word \n",
    "        representations can be any length.)\n",
    "        \n",
    "    hidden_dim : int (default: 10)\n",
    "        Dimensionality of the hidden representations. This is a\n",
    "        parameter to `ClassifierRNN`.\n",
    "        \n",
    "    eta : float (default: 0.05)\n",
    "        The learning rate. This is a parameter to `ClassifierRNN`.       \n",
    "        \n",
    "    maxiter : int (default: 10)\n",
    "        Maximum number of training epochs. This is a parameter \n",
    "        to `ClassifierRNN`.       \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A formatted `sklearn` `classification_report`.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Training:\n",
    "    train = build_rnn_dataset(train_reader)       \n",
    "    mod = ClassifierRNN(\n",
    "        vocab, \n",
    "        embedding, \n",
    "        hidden_dim=hidden_dim, \n",
    "        eta=eta,\n",
    "        maxiter=maxiter)\n",
    "    mod.fit(train)    \n",
    "    # Assessment:\n",
    "    assess = build_rnn_dataset(dev_reader) \n",
    "    return rnn_model_evaluation(mod, assess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_model_evaluation(mod, assess):\n",
    "    \"\"\"Asssess a trained `ClassifierRNN`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mod : `ClassifierRNN`\n",
    "        Should be a model trained on data in the same format as\n",
    "        `assess`.\n",
    "    \n",
    "    assess : list\n",
    "        A list of (seq, label) pairs, where seq is a sequence of\n",
    "        words and label is a one-hot vector giving the label.        \n",
    "    \n",
    "    \"\"\"    \n",
    "    # Assessment:\n",
    "    gold = []\n",
    "    predictions = []    \n",
    "    for seq, y_ in assess:\n",
    "        # The gold labels are vectors. Get the index of the single 1\n",
    "        # and look up its string in `LABELS`:\n",
    "        gold.append(LABELS[np.argmax(y_)])\n",
    "        # `predict` returns the index of the highest score.\n",
    "        p = mod.predict(seq) \n",
    "        predictions.append(LABELS[p])\n",
    "    # Report:\n",
    "    return classification_report(gold, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example run. All input and hidden dimensions are quite small, as is `maxiter`. This is just so you can run experiments quickly and see what happens. Nonetheless, the performance is competitive with the linear classifier above, which is encouraging about this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 1.08155358503"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.36      0.25      0.29       500\n",
      " entailment       0.40      0.59      0.47       500\n",
      "    neutral       0.40      0.34      0.37       500\n",
      "\n",
      "avg / total       0.39      0.39      0.38      1500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = snli_sample['vocab']\n",
    "\n",
    "# Random embeddings of dimension 10:\n",
    "randvec_embedding = np.array([utils.randvec(10) for w in vocab])\n",
    "\n",
    "# A small network, trained for just a few epochs to see how things look:\n",
    "print(rnn_experiment(vocab, randvec_embedding, hidden_dim=10, eta=0.001, maxiter=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps for NLI deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, `ClassifierRNN` is just about the simplest model we could use for this task. Some thoughts on where to take it:\n",
    "\n",
    "* Additional hidden layers can be added. This is a relatively simple change to the code: one just needs to define a version of $W_{hh}$ for each layer, respecting the desired dimensions for the  representations of the layers it connects. The backpropagation steps are also straightforward duplications of what happens between the current layers.\n",
    "\n",
    "* `ClassifierRNN` uses the most basic (non-linear) activation functions. In TensorFlow, it is easy to try more advanced designs, including Long Short-Term Memory (LSTM) cells and Gated Recurrent Unit (GRU) cells. The documentation for these is currently a bit hard to find, but here's [the well-documented source code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py).\n",
    "\n",
    "* The [SNLI leaderboard](http://nlp.stanford.edu/projects/snli/) shows the value of adding __attention__ layers. These are additional connections between premise and hypothesis. They can be made for each pair of words or just for the final hidden representation in the premise and hypothesis.\n",
    "\n",
    "* We haven't made good use of trees. Like many linguists, I believe trees are necessary for capturing the nuanced ways in which we reason in language, and [this new paper](http://arxiv.org/abs/1603.06021) offers  empirical evidence that trees are important for SNLI. Tree-structured neural networks are by now well-understood extensions of feed-forward neural networks and so are well within reach for a final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional NLI resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do get [the full SNLI](http://nlp.stanford.edu/projects/snli/) and figure out how to grapple with its large size! Here's [a useful and insightful blog post by Sam Bowman on SNLI's design](http://nlp.stanford.edu/blog/the-stanford-nli-corpus-revisited/).\n",
    "\n",
    "* The folder [nli-data](nli-data) in this repository contains the NLI data from the [SemEval 2014 semantic relatedness task](http://alt.qcri.org/semeval2014/task1/). \n",
    "This data set is called \"Sentences Involving Compositional Knowledge\" or, for better or worse, \n",
    "\"SICK\". It's [freely available from the SemEval site](http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools).  [nli-data](nli-data) contains a parsed version created by [Sam Bowman](http://stanford.edu/~sbowman/) \n",
    "as part of [his research on neural models of semantic composition](https://github.com/sleepinyourhat/vector-entailment/releases/tag/W15-R1). \n",
    "\n",
    "* [SemEval 2013](https://www.cs.york.ac.uk/semeval-2013/) also had a wide range of interesting data sets for NLI and related tasks.\n",
    "\n",
    "* The [FraCaS textual inference test suite](http://www-nlp.stanford.edu/~wcmac/downloads/) is a smaller, hand-built dataset that is great for evaluating a model's ability to handle complex logical patterns.\n",
    "\n",
    "* Models for NLI might be adapted for use with the [30M Factoid Question-Answer Corpus](http://agarciaduran.org).\n",
    "\n",
    "* Models for NLI might be adapted for use with the [Penn Paraphrase Database](http://paraphrase.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. WordNet-based entailment features [4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python NLTK](http://www.nltk.org) has an excellent WordNet interface. As noted above, WordNet is a natural choice for defining useful features in the context of NLI.\n",
    "\n",
    "__Your task__: write and submit a feature function, for use with `build_linear_classifier_dataset`, that is just like `word_cross_product_phi` except that, given a sentence pair $(S_{1}, S_{2})$, it counts only pairs $(w_{1}, w_{2})$ such that $w_{1}$ entails $w_{2}$, for $w_{1} \\in S_{1}$ and $w_{2} \\in S_{2}$. For example, the sentence pair (_the cat runs_, _the animal moves_) would create the dictionary `{(cat, animal): 1.0, (runs, moves): 1.0}`.\n",
    "\n",
    "There are many ways to do this. For the purposes of the question, we can limit attention to the WordNet hypernym relation. The following illustrates reasonable ways to go from a string $s$ to the set of all hypernyms of Synsets consistent with $s$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('dog.n.01'), Synset('pup.n.01'), Synset('young_person.n.01')]\n",
      "[Synset('dog.n.01'), Synset('pup.n.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "    \n",
    "puppies = wn.synsets('puppy')\n",
    "print([h for ss in puppies for h in ss.hypernyms()])\n",
    "\n",
    "# A more conservative approach uses just the first-listed \n",
    "# Synset, which should be the most frequent sense:\n",
    "print(wn.synsets('puppy')[0].hypernyms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pretrained RNN inputs [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the simple RNN experiment above, we used random input vectors. In the [word-entailment bake-off](wordentail.ipynb), pretraining was clearly beneficial. What are the effects of using pretrained inputs here?\n",
    "\n",
    "__Submit__:\n",
    "\n",
    "0. A function `build_glove_embedding` that creates an embedding space for all of the words in `snli_sample['vocab']`.  (You can use any GloVe file you like; the `50d` one will be fastest.) See `randvec_embedding` above if you need further guidance on the nature of the data structure to produce. If you encounter any words in `snli_sample['vocab']` that are not in GloVe, have your function map them instead to a random vector of the appropriate dimensionality (see `utils.randvec`).\n",
    "\n",
    "0. A function call for `rnn_experiment` using your GloVe embedding. (You can set the other parameters to `rnn_experiment` however you like.)\n",
    "\n",
    "0. The output of this function. (You won't be evaluated by how strong the performance is. We're just curious.)\n",
    "\n",
    "You can use `utils.glove2dict` to read in the GloVe data into a `dict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Simulating a propositional logic theorem prover [4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can our simple RNN learn to simulate a propositional logic theorem prover? This gets at the more general question of the extent to which deep learning models can learn to simulate symbolic systems.\n",
    "\n",
    "The file `pl_theorems.pickle` contains triples of the form $(\\varphi, \\psi, y)$, where $\\varphi$ and $\\psi$ are propositional logic formulae and $y$ is `entailment`, `contradiction`, or `neutral`, just as in our SNLI task. In this case, the label is determined by the model theory of propositional logic. The triples look like this:\n",
    "\n",
    "`(\"(p&q)\", \"p\", 'entailment')`\n",
    "\n",
    "`(\"~p\", \"(p&q)\", 'contradiction')`\n",
    "\n",
    "`(\"~p\", \"(p|q)\", 'neutral')`\n",
    "\n",
    "Using simulated data in this way gives us a  clear learning target.\n",
    "\n",
    "__Submit__:\n",
    "\n",
    "0. A function `pl_splits` that randomly splits the contents of `pl_theorems.pickle` into a train set and a test set. Be sure to attend to the labels &ndash; they need to be turned from strings into one-hot vectors (see `build_rnn_dataset` above for the method).\n",
    "\n",
    "0. A function `pl_rnn_experiment`, comparable to `rnn_experiment`, that uses your splits to train and assess a `ClassifierRNN`. Some notes and requirements for this:\n",
    "\n",
    "  * The formulae are designed so that each character is a meaningful unit. So `list(phi)` is the list of \"words\" in the formula `phi`.\n",
    "  * Use `rnn_model_evaluation` for assessment.\n",
    "  * You can use random embeddings. \n",
    "  * It's probably good to give the user control over important model hyperparameters like `eta`, `hidden_dim`, the dimensionality of the inputs, etc. This isn't required, though.\n",
    "  \n",
    "__Practical note__: Be sure to set `eta` low. High values often lead to unstable learning for this problem.\n",
    "\n",
    "__Conceptual note__: the formulae have a clear tree structure, as indicated by their bracketing, but we are processing them as a strings. Part of what we are assessing is the extent to which this purely sequence-based model can learn that the brackets indicate subformulae, and keep track of how those subformulae are delimited.\n",
    "\n",
    "Here's some code to help you structure your answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Loads the data:\n",
    "pl = pickle.load(open('pl_theorems.pickle', 'rb'))\n",
    "\n",
    "# The connectives:\n",
    "NEG = \"~\"\n",
    "AND = \"&\"\n",
    "OR = \"|\"\n",
    "IMPLIES = \"<\"\n",
    "IFF = \"=\"\n",
    "\n",
    "# The full vocabulary:\n",
    "pl_vocab = [NEG, AND, OR, IMPLIES, IFF, \"(\", \")\", \"p\", \"q\"]    \n",
    "\n",
    "def pl_splits(pl, train_size=0.7):\n",
    "    \"\"\"Randomly divide `pl` into train and test sets based on `train_size`\"\"\"\n",
    "    dataset = []    \n",
    "    # Build the dataset as required by `ClassifierRNN`:\n",
    "    \n",
    "    # Rely on `sklearn` for the random split:\n",
    "    train, assess = train_test_split(dataset, train_size=train_size)                        \n",
    "    return (train, assess)\n",
    "\n",
    "def pl_rnn_experiment(pl, word_dim=5, hidden_dim=5, eta=0.0001, maxiter=10):    \n",
    "    \"\"\"Propositional logic experiments.\"\"\"\n",
    "    # Split `pl` using `pl_splits`:\n",
    "    train, assess = pl_splits(pl)  \n",
    "    # Create an embedding:\n",
    "\n",
    "    # Instantiate and fit a `ClassifierRNN`:\n",
    "\n",
    "    # Use `rnn_model_evaluation` to assess the results:\n",
    "\n",
    "\n",
    "#print(pl_rnn_experiment(pl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: If you make progress on this version of the problem, then the best next step would be to create splits in which the assessment formulae are  longer than those seen in training. This will provide a clear estimate of how much the model is actually generalizing to totally unseen formula &ndash; how much the model is actually starting to simulate a theorem prover. For encouraging results about such problems, see [Bowman et al. 2015](http://arxiv.org/abs/1406.1827)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
